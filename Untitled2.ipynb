{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pdb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trixi.logger.experiment.pytorchexperimentlogger import PytorchExperimentLogger\n",
    "from trixi.logger import PytorchVisdomLogger\n",
    "from trixi.util import Config\n",
    "\n",
    "Exp = PytorchExperimentLogger(base_dir=\"./experiment_dir\", \n",
    "                              experiment_name=\"test-experiment\",\n",
    "                              folder_format=\"{experiment_name}\")\n",
    "\n",
    "Viz = PytorchVisdomLogger(name=\"main\", port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arg():\n",
    "    seed=1\n",
    "    no_cuda=True\n",
    "    batch_size=100\n",
    "    intermediate_size=128 #usual hidden size, linear around z\n",
    "    hidden_size=30 # latent space z\n",
    "    test_batch_size=100\n",
    "    epochs=10\n",
    "    lr=1e-1 #0.001\n",
    "    momentum=0.5\n",
    "    log_interval=10\n",
    "    save_model=True\n",
    "        \n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 16 * 32, 128)#FC 128\n",
    "        #F*((Iâˆ’K+2P)/S+1)\n",
    "        \n",
    "        # Latent space\n",
    "        self.fc21 = nn.Linear(128, 20)\n",
    "        self.fc22 = nn.Linear(128, 20)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(20, 128)\n",
    "        self.fc4 = nn.Linear(128, 8192)\n",
    "        self.deconv1 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 32, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv5 = nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.relu(self.conv3(out))\n",
    "        out = self.relu(self.conv4(out))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        h1 = self.relu(self.fc1(out))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        out = self.relu(self.fc4(h3))\n",
    "        # import pdb; pdb.set_trace()\n",
    "        out = out.view(out.size(0), 32, 16, 16)\n",
    "        out = self.relu(self.deconv1(out))\n",
    "        out = self.relu(self.deconv2(out))\n",
    "        out = self.relu(self.deconv3(out))\n",
    "        out = self.sigmoid(self.conv5(out))\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x) \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def main3():\n",
    "    args=arg()\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if not args.no_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=True, download=True,\n",
    "                     transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('../data', train=False, transform=transforms.ToTensor()),\n",
    "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "    \n",
    "    #https://stackoverflow.com/questions/37512290/reading-cifar10-dataset-in-batches\n",
    "    data_iter = iter(train_loader)\n",
    "    \n",
    "    #how to save fixed inputs for debugging\n",
    "    fixed_x, _ = next(data_iter)\n",
    "    save_image(Variable(fixed_x).data.cpu(), './data/vae_real_images.png')\n",
    "    #args.fixed_x = to_var(fixed_x.view(fixed_x.size(0), -1)) #nur erstes batch i think als baeline\n",
    "    args.fixed_x = Variable(fixed_x, volatile=True)\n",
    "    #args.fixed_x = to_var(fixed_x)\n",
    "    model = VAE()\n",
    "    if not args.no_cuda:\n",
    "        model.cuda()\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "        \n",
    "    def loss_function(recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x.view(-1, 32 * 32 * 3),\n",
    "                                 x.view(-1, 32 * 32 * 3), size_average=False)\n",
    "\n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # https://arxiv.org/abs/1312.6114\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        #kullbach-leibler divergence\n",
    "        return BCE + KLD\n",
    "\n",
    "\n",
    "    def train(epoch):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            data = Variable(data)\n",
    "            if not args.no_cuda:\n",
    "                data = data.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.item() / len(data)))\n",
    "\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "              epoch, train_loss / len(train_loader.dataset)))\n",
    "        \n",
    "        # save the reconstructed images\n",
    "        reconst_images, mu, logvar  = model(args.fixed_x)\n",
    "        reconst_images = reconst_images.view(reconst_images.size(0), 3, 32, 32)\n",
    "        save_image(reconst_images.data.cpu(), './data/vae_reconst_images_%d.png' % (epoch))\n",
    "\n",
    "\n",
    "    def test(epoch):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            if not args.no_cuda:\n",
    "                data = data.cuda()\n",
    "            data = Variable(data, volatile=True)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            '''            if epoch == args.epochs and i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                       recon_batch[:n]])\n",
    "                save_image(comparison.data.cpu(),\n",
    "                           'snapshots/conv_vae/reconstruction_' + str(epoch) +\n",
    "                           '.png', nrow=n)'''\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        '''if epoch == args.epochs:\n",
    "            sample = Variable(torch.randn(64, args.hidden_size))\n",
    "            if not args.no_cuda:\n",
    "                sample = sample.cuda()\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.data.view(64, 3, 32, 32),\n",
    "                       'snapshots/conv_vae/sample_' + str(epoch) + '.png')'''\n",
    "        torch.save(model.state_dict(), \"vae_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main4(load_old=False):\n",
    "    \n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            '''old: self.conv1 = nn.Conv2d(1, 20, 5, 1)#in out kernel_sz stride\n",
    "            self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "            self.fc1 = nn.Linear(4 * 4 * 50, 500) # in out\n",
    "            self.fc2 = nn.Linear(500, 10)'''\n",
    "             # Encoder\n",
    "            self.conv1 = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv2 = nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0)\n",
    "            self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv4 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "            self.fc1 = nn.Linear(16 * 16 * 32, 128)#FC 128\n",
    "            #F*((Iâˆ’K+2P)/S+1)\n",
    "            \n",
    "            '''# Latent space\n",
    "            self.fc21 = nn.Linear(128, 20)\n",
    "            self.fc22 = nn.Linear(128, 20)'''\n",
    "            # Decoder\n",
    "            '''self.fc3 = nn.Linear(20, 128)'''\n",
    "            self.fc4 = nn.Linear(128, 8192)\n",
    "            self.deconv1 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "            self.deconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "            self.deconv3 = nn.ConvTranspose2d(32, 32, kernel_size=2, stride=2, padding=0)\n",
    "            self.conv5 = nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1)\n",
    "            \n",
    "            self.relu = nn.ReLU()\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "        def encode(self, x):\n",
    "            x = F.relu(self.conv1(x))\n",
    "            #x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            #x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.relu(self.conv3(x))\n",
    "            #x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.relu(self.conv4(x))\n",
    "            #x = F.max_pool2d(x, 2, 2)\n",
    "            \n",
    "            x = x.view(x.size(0),-1)\n",
    "            \n",
    "            return F.relu(self.fc1(x))\n",
    "        def normalize(self, x):\n",
    "            #x_normed = x / x.max(0, keepdim=True)[0] \n",
    "            #return x_normed\n",
    "            alpha=(x-x.mean(0,keepdim=True))\n",
    "            beta=alpha/x.std(0,keepdim=True)\n",
    "            return beta\n",
    "            \n",
    "        def decode(self, x):\n",
    "            out = self.relu(self.fc4(x))\n",
    "            # import pdb; pdb.set_trace()\n",
    "            out = out.view(out.size(0), 32, 16, 16)\n",
    "            out = self.relu(self.deconv1(out))\n",
    "            out = self.relu(self.deconv2(out))\n",
    "            out = self.relu(self.deconv3(out))\n",
    "            out = self.sigmoid(self.conv5(out))\n",
    "            return out\n",
    "            \n",
    "        def forward(self, x):\n",
    "            mu = self.encode(x)\n",
    "            #return F.log_softmax(x, dim=1)\n",
    "            mu_0=self.normalize(mu)\n",
    "            #return self.decode(mu_0),mu_0\n",
    "            return self.decode(mu),mu\n",
    "        \n",
    "    '''def loss_function(recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x.view(-1, 32 * 32 * 3),\n",
    "                                 x.view(-1, 32 * 32 * 3), size_average=False)\n",
    "\n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # https://arxiv.org/abs/1312.6114\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        #kullbach-leibler divergence\n",
    "        return BCE + KLD'''\n",
    "    \n",
    "    def loss_function(recon_x, x):\n",
    "        #pdb.set_trace()\n",
    "        BCE = F.binary_cross_entropy(recon_x.view(-1, 32 * 32 * 3),\n",
    "                                 x.view(-1, 32 * 32 * 3), size_average=False)\n",
    "        return BCE \n",
    "    \n",
    "    def train(args, model, device, train_loader, optimizer, epoch):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)[0]\n",
    "            loss = loss_function(output, data)\n",
    "            loss.backward()\n",
    "            \n",
    "            #loss = F.nll_loss(output, target)\n",
    "            #loss.backward()\n",
    "            \n",
    "            #train_loss += loss.data[0]\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            optimizer.step()\n",
    "            '''if batch_idx % args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()))'''\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.item() / len(data)))\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "        # save the reconstructed images\n",
    "        reconst_images = model(args.fixed_x)[0]\n",
    "        \n",
    "        ##\n",
    "        #print(my_variable.data.cpu().numpy())\n",
    "        #x = Variable()\n",
    "        #print(np.shape(reconst_images.data.cpu().numpy()[0]))\n",
    "        reconst_images = reconst_images.view(reconst_images.size(0), 3, 32, 32)\n",
    "        save_image(reconst_images.data.cpu(), './data/CIFAR_reconst_images_%d.png' % (epoch))\n",
    "\n",
    "    def test(args, model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        #correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)[0]\n",
    "                #nochma angucken\n",
    "                #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss, negative log likelihood loss.\n",
    "                \n",
    "                test_loss += loss_function(output, data).item()\n",
    "                #pdb.set_trace()\n",
    "                pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "                #correct += pred.eq(target.view_as(pred)).sum().item() #accuracy, elementwise equality, and sum\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        print('\\nTest set: Average loss: {:.4f}\\n'.format(\n",
    "            test_loss))\n",
    "    \n",
    "    \n",
    "    args=arg()  \n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=True, download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    if load_old:\n",
    "        model=Net().to(device)\n",
    "        model.load_state_dict(torch.load(\"cifar_cnn_12.pt\"))\n",
    "    else:\n",
    "        model = Net().to(device)\n",
    "    \n",
    "    #model = Net()\n",
    "    #if not args.no_cuda:\n",
    "    #    model.cuda()\n",
    "        \n",
    "    #optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "    #gradients tend to vanish or explode\n",
    "    '''It uses a moving average of squared gradients to normalize the gradient itself. \n",
    "    That has an effect of balancing the step sizeâ€Šâ€”â€Šdecrease the step for large gradient \n",
    "    to avoid exploding, and \n",
    "    increase the step for small gradient to avoid vanishing'''\n",
    "    \n",
    "    \n",
    "    #how to save fixed inputs for debugging\n",
    "    data_iter = iter(train_loader)\n",
    "    fixed_x, _ = next(data_iter)\n",
    "    #pdb.set_trace()\n",
    "    save_image(Variable(fixed_x).data.cpu(), './data/CIFAR_real_images.png')\n",
    "    args.fixed_x = to_var(fixed_x) \n",
    "    #args.fixed_x = to_var(fixed_x.view(fixed_x.size(0), -1)) \n",
    "    #args.fixed_x=args.fixed_x.to(device)\n",
    "    #print(np.shape(args.fixed_x.data.cpu().numpy()))\n",
    "    \n",
    "    for epoch in range(12, args.epochs + 12):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader)\n",
    "\n",
    "        if (args.save_model):\n",
    "            torch.save(model.state_dict(), f\"cifar_cnn_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2127.722500\n",
      "Train Epoch: 1 [1000/50000 (2%)]\tLoss: 2116.976250\n",
      "Train Epoch: 1 [2000/50000 (4%)]\tLoss: 2118.335000\n",
      "Train Epoch: 1 [3000/50000 (6%)]\tLoss: 2098.944219\n",
      "Train Epoch: 1 [4000/50000 (8%)]\tLoss: 2182.098125\n",
      "Train Epoch: 1 [5000/50000 (10%)]\tLoss: 2099.554531\n",
      "Train Epoch: 1 [6000/50000 (12%)]\tLoss: 2001.332031\n",
      "Train Epoch: 1 [7000/50000 (14%)]\tLoss: 1991.417813\n",
      "Train Epoch: 1 [8000/50000 (16%)]\tLoss: 2000.685469\n",
      "Train Epoch: 1 [9000/50000 (18%)]\tLoss: 2011.515938\n",
      "Train Epoch: 1 [10000/50000 (20%)]\tLoss: 2013.734688\n",
      "Train Epoch: 1 [11000/50000 (22%)]\tLoss: 2005.849062\n",
      "Train Epoch: 1 [12000/50000 (24%)]\tLoss: 1937.953750\n",
      "Train Epoch: 1 [13000/50000 (26%)]\tLoss: 2013.366875\n",
      "Train Epoch: 1 [14000/50000 (28%)]\tLoss: 1899.362656\n",
      "Train Epoch: 1 [15000/50000 (30%)]\tLoss: 1923.627656\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 1902.885313\n",
      "Train Epoch: 1 [17000/50000 (34%)]\tLoss: 1920.427187\n",
      "Train Epoch: 1 [18000/50000 (36%)]\tLoss: 1886.350312\n",
      "Train Epoch: 1 [19000/50000 (38%)]\tLoss: 1956.141875\n",
      "Train Epoch: 1 [20000/50000 (40%)]\tLoss: 1889.157344\n",
      "Train Epoch: 1 [21000/50000 (42%)]\tLoss: 1903.288281\n",
      "Train Epoch: 1 [22000/50000 (44%)]\tLoss: 1947.425156\n",
      "Train Epoch: 1 [23000/50000 (46%)]\tLoss: 1908.129531\n",
      "Train Epoch: 1 [24000/50000 (48%)]\tLoss: 1934.980937\n",
      "Train Epoch: 1 [25000/50000 (50%)]\tLoss: 1883.023125\n",
      "Train Epoch: 1 [26000/50000 (52%)]\tLoss: 1864.879844\n",
      "Train Epoch: 1 [27000/50000 (54%)]\tLoss: 1913.997813\n",
      "Train Epoch: 1 [28000/50000 (56%)]\tLoss: 1889.348281\n",
      "Train Epoch: 1 [29000/50000 (58%)]\tLoss: 1910.973438\n",
      "Train Epoch: 1 [30000/50000 (60%)]\tLoss: 1873.123594\n",
      "Train Epoch: 1 [31000/50000 (62%)]\tLoss: 1876.423125\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1896.936406\n",
      "Train Epoch: 1 [33000/50000 (66%)]\tLoss: 1904.654844\n",
      "Train Epoch: 1 [34000/50000 (68%)]\tLoss: 1847.171250\n",
      "Train Epoch: 1 [35000/50000 (70%)]\tLoss: 1877.156094\n",
      "Train Epoch: 1 [36000/50000 (72%)]\tLoss: 1870.517969\n",
      "Train Epoch: 1 [37000/50000 (74%)]\tLoss: 1856.352969\n",
      "Train Epoch: 1 [38000/50000 (76%)]\tLoss: 1860.614531\n",
      "Train Epoch: 1 [39000/50000 (78%)]\tLoss: 1813.830625\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 1898.252812\n",
      "Train Epoch: 1 [41000/50000 (82%)]\tLoss: 1876.550469\n",
      "Train Epoch: 1 [42000/50000 (84%)]\tLoss: 1860.488437\n",
      "Train Epoch: 1 [43000/50000 (86%)]\tLoss: 1845.268125\n",
      "Train Epoch: 1 [44000/50000 (88%)]\tLoss: 1894.339844\n",
      "Train Epoch: 1 [45000/50000 (90%)]\tLoss: 1888.706875\n",
      "Train Epoch: 1 [46000/50000 (92%)]\tLoss: 1893.537188\n",
      "Train Epoch: 1 [47000/50000 (94%)]\tLoss: 1902.481094\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1901.236562\n",
      "Train Epoch: 1 [49000/50000 (98%)]\tLoss: 1890.706719\n",
      "====> Epoch: 1 Average loss: 1932.5782\n",
      "\n",
      "Test set: Average loss: 1870.0265\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1888.756094\n",
      "Train Epoch: 2 [1000/50000 (2%)]\tLoss: 1838.607656\n",
      "Train Epoch: 2 [2000/50000 (4%)]\tLoss: 1878.343281\n",
      "Train Epoch: 2 [3000/50000 (6%)]\tLoss: 1824.676250\n",
      "Train Epoch: 2 [4000/50000 (8%)]\tLoss: 1825.482188\n",
      "Train Epoch: 2 [5000/50000 (10%)]\tLoss: 1880.422031\n",
      "Train Epoch: 2 [6000/50000 (12%)]\tLoss: 1897.203438\n",
      "Train Epoch: 2 [7000/50000 (14%)]\tLoss: 1830.407813\n",
      "Train Epoch: 2 [8000/50000 (16%)]\tLoss: 1849.168437\n",
      "Train Epoch: 2 [9000/50000 (18%)]\tLoss: 1848.778281\n",
      "Train Epoch: 2 [10000/50000 (20%)]\tLoss: 1841.003750\n",
      "Train Epoch: 2 [11000/50000 (22%)]\tLoss: 1916.561719\n",
      "Train Epoch: 2 [12000/50000 (24%)]\tLoss: 1855.059375\n",
      "Train Epoch: 2 [13000/50000 (26%)]\tLoss: 1857.536250\n",
      "Train Epoch: 2 [14000/50000 (28%)]\tLoss: 1815.886875\n",
      "Train Epoch: 2 [15000/50000 (30%)]\tLoss: 1857.389219\n",
      "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 1834.441719\n",
      "Train Epoch: 2 [17000/50000 (34%)]\tLoss: 1853.718125\n",
      "Train Epoch: 2 [18000/50000 (36%)]\tLoss: 1852.247813\n",
      "Train Epoch: 2 [19000/50000 (38%)]\tLoss: 1828.996875\n",
      "Train Epoch: 2 [20000/50000 (40%)]\tLoss: 1850.170000\n",
      "Train Epoch: 2 [21000/50000 (42%)]\tLoss: 1795.483281\n",
      "Train Epoch: 2 [22000/50000 (44%)]\tLoss: 1847.460625\n",
      "Train Epoch: 2 [23000/50000 (46%)]\tLoss: 1895.276875\n",
      "Train Epoch: 2 [24000/50000 (48%)]\tLoss: 1842.104687\n",
      "Train Epoch: 2 [25000/50000 (50%)]\tLoss: 1838.114063\n",
      "Train Epoch: 2 [26000/50000 (52%)]\tLoss: 1880.881719\n",
      "Train Epoch: 2 [27000/50000 (54%)]\tLoss: 1795.325625\n",
      "Train Epoch: 2 [28000/50000 (56%)]\tLoss: 1843.888750\n",
      "Train Epoch: 2 [29000/50000 (58%)]\tLoss: 1872.794063\n",
      "Train Epoch: 2 [30000/50000 (60%)]\tLoss: 1850.165000\n",
      "Train Epoch: 2 [31000/50000 (62%)]\tLoss: 1849.052031\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1866.456406\n",
      "Train Epoch: 2 [33000/50000 (66%)]\tLoss: 1778.826406\n",
      "Train Epoch: 2 [34000/50000 (68%)]\tLoss: 1829.325313\n",
      "Train Epoch: 2 [35000/50000 (70%)]\tLoss: 1879.897813\n",
      "Train Epoch: 2 [36000/50000 (72%)]\tLoss: 1813.633906\n",
      "Train Epoch: 2 [37000/50000 (74%)]\tLoss: 1863.452031\n",
      "Train Epoch: 2 [38000/50000 (76%)]\tLoss: 1869.298125\n",
      "Train Epoch: 2 [39000/50000 (78%)]\tLoss: 1799.965000\n",
      "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 1848.109219\n",
      "Train Epoch: 2 [41000/50000 (82%)]\tLoss: 1862.564531\n",
      "Train Epoch: 2 [42000/50000 (84%)]\tLoss: 1804.941562\n",
      "Train Epoch: 2 [43000/50000 (86%)]\tLoss: 1835.532969\n",
      "Train Epoch: 2 [44000/50000 (88%)]\tLoss: 1842.273281\n",
      "Train Epoch: 2 [45000/50000 (90%)]\tLoss: 1834.662031\n",
      "Train Epoch: 2 [46000/50000 (92%)]\tLoss: 1806.751875\n",
      "Train Epoch: 2 [47000/50000 (94%)]\tLoss: 1854.435937\n",
      "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1812.336875\n",
      "Train Epoch: 2 [49000/50000 (98%)]\tLoss: 1793.590156\n",
      "====> Epoch: 2 Average loss: 1844.8794\n",
      "\n",
      "Test set: Average loss: 1830.1261\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1811.978437\n",
      "Train Epoch: 3 [1000/50000 (2%)]\tLoss: 1819.668437\n",
      "Train Epoch: 3 [2000/50000 (4%)]\tLoss: 1813.072187\n",
      "Train Epoch: 3 [3000/50000 (6%)]\tLoss: 1819.769375\n",
      "Train Epoch: 3 [4000/50000 (8%)]\tLoss: 1838.904219\n",
      "Train Epoch: 3 [5000/50000 (10%)]\tLoss: 1810.347500\n",
      "Train Epoch: 3 [6000/50000 (12%)]\tLoss: 1868.409688\n",
      "Train Epoch: 3 [7000/50000 (14%)]\tLoss: 1825.531250\n",
      "Train Epoch: 3 [8000/50000 (16%)]\tLoss: 1822.101406\n",
      "Train Epoch: 3 [9000/50000 (18%)]\tLoss: 1841.745781\n",
      "Train Epoch: 3 [10000/50000 (20%)]\tLoss: 1790.819844\n",
      "Train Epoch: 3 [11000/50000 (22%)]\tLoss: 1821.790000\n",
      "Train Epoch: 3 [12000/50000 (24%)]\tLoss: 1835.249531\n",
      "Train Epoch: 3 [13000/50000 (26%)]\tLoss: 1834.270469\n",
      "Train Epoch: 3 [14000/50000 (28%)]\tLoss: 1855.677969\n",
      "Train Epoch: 3 [15000/50000 (30%)]\tLoss: 1820.225000\n",
      "Train Epoch: 3 [16000/50000 (32%)]\tLoss: 1822.135156\n",
      "Train Epoch: 3 [17000/50000 (34%)]\tLoss: 1860.470781\n",
      "Train Epoch: 3 [18000/50000 (36%)]\tLoss: 1856.451719\n",
      "Train Epoch: 3 [19000/50000 (38%)]\tLoss: 1833.115313\n",
      "Train Epoch: 3 [20000/50000 (40%)]\tLoss: 1816.108906\n",
      "Train Epoch: 3 [21000/50000 (42%)]\tLoss: 1795.356406\n",
      "Train Epoch: 3 [22000/50000 (44%)]\tLoss: 1859.991563\n",
      "Train Epoch: 3 [23000/50000 (46%)]\tLoss: 1831.478281\n",
      "Train Epoch: 3 [24000/50000 (48%)]\tLoss: 1782.917344\n",
      "Train Epoch: 3 [25000/50000 (50%)]\tLoss: 1833.895313\n",
      "Train Epoch: 3 [26000/50000 (52%)]\tLoss: 1842.425156\n",
      "Train Epoch: 3 [27000/50000 (54%)]\tLoss: 1818.414375\n",
      "Train Epoch: 3 [28000/50000 (56%)]\tLoss: 1818.455156\n",
      "Train Epoch: 3 [29000/50000 (58%)]\tLoss: 1811.394063\n",
      "Train Epoch: 3 [30000/50000 (60%)]\tLoss: 1839.075000\n",
      "Train Epoch: 3 [31000/50000 (62%)]\tLoss: 1859.969531\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1763.876094\n",
      "Train Epoch: 3 [33000/50000 (66%)]\tLoss: 1801.501719\n",
      "Train Epoch: 3 [34000/50000 (68%)]\tLoss: 1817.532500\n",
      "Train Epoch: 3 [35000/50000 (70%)]\tLoss: 1818.671250\n",
      "Train Epoch: 3 [36000/50000 (72%)]\tLoss: 1819.915937\n",
      "Train Epoch: 3 [37000/50000 (74%)]\tLoss: 1845.695937\n",
      "Train Epoch: 3 [38000/50000 (76%)]\tLoss: 1799.505156\n",
      "Train Epoch: 3 [39000/50000 (78%)]\tLoss: 1822.922969\n",
      "Train Epoch: 3 [40000/50000 (80%)]\tLoss: 1822.302187\n",
      "Train Epoch: 3 [41000/50000 (82%)]\tLoss: 1822.677031\n",
      "Train Epoch: 3 [42000/50000 (84%)]\tLoss: 1766.711719\n",
      "Train Epoch: 3 [43000/50000 (86%)]\tLoss: 1815.468437\n",
      "Train Epoch: 3 [44000/50000 (88%)]\tLoss: 1817.518750\n",
      "Train Epoch: 3 [45000/50000 (90%)]\tLoss: 1818.373281\n",
      "Train Epoch: 3 [46000/50000 (92%)]\tLoss: 1817.225938\n",
      "Train Epoch: 3 [47000/50000 (94%)]\tLoss: 1801.980313\n",
      "Train Epoch: 3 [48000/50000 (96%)]\tLoss: 1790.107812\n",
      "Train Epoch: 3 [49000/50000 (98%)]\tLoss: 1832.275625\n",
      "====> Epoch: 3 Average loss: 1823.1202\n",
      "\n",
      "Test set: Average loss: 1803.8069\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1803.121250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [1000/50000 (2%)]\tLoss: 1778.044688\n",
      "Train Epoch: 4 [2000/50000 (4%)]\tLoss: 1805.629219\n",
      "Train Epoch: 4 [3000/50000 (6%)]\tLoss: 1811.555469\n",
      "Train Epoch: 4 [4000/50000 (8%)]\tLoss: 1823.006719\n",
      "Train Epoch: 4 [5000/50000 (10%)]\tLoss: 1798.974375\n",
      "Train Epoch: 4 [6000/50000 (12%)]\tLoss: 1785.298750\n",
      "Train Epoch: 4 [7000/50000 (14%)]\tLoss: 1843.603437\n",
      "Train Epoch: 4 [8000/50000 (16%)]\tLoss: 1822.619062\n",
      "Train Epoch: 4 [9000/50000 (18%)]\tLoss: 1804.399688\n",
      "Train Epoch: 4 [10000/50000 (20%)]\tLoss: 1830.427031\n",
      "Train Epoch: 4 [11000/50000 (22%)]\tLoss: 1825.886406\n",
      "Train Epoch: 4 [12000/50000 (24%)]\tLoss: 1829.982188\n",
      "Train Epoch: 4 [13000/50000 (26%)]\tLoss: 1765.287812\n",
      "Train Epoch: 4 [14000/50000 (28%)]\tLoss: 1810.230937\n",
      "Train Epoch: 4 [15000/50000 (30%)]\tLoss: 1805.215469\n",
      "Train Epoch: 4 [16000/50000 (32%)]\tLoss: 1797.407187\n",
      "Train Epoch: 4 [17000/50000 (34%)]\tLoss: 1812.321094\n",
      "Train Epoch: 4 [18000/50000 (36%)]\tLoss: 1782.374687\n",
      "Train Epoch: 4 [19000/50000 (38%)]\tLoss: 1800.274219\n",
      "Train Epoch: 4 [20000/50000 (40%)]\tLoss: 1790.119688\n",
      "Train Epoch: 4 [21000/50000 (42%)]\tLoss: 1790.566094\n",
      "Train Epoch: 4 [22000/50000 (44%)]\tLoss: 1780.771719\n",
      "Train Epoch: 4 [23000/50000 (46%)]\tLoss: 1788.550313\n",
      "Train Epoch: 4 [24000/50000 (48%)]\tLoss: 1825.842656\n",
      "Train Epoch: 4 [25000/50000 (50%)]\tLoss: 1776.553594\n",
      "Train Epoch: 4 [26000/50000 (52%)]\tLoss: 1739.550000\n",
      "Train Epoch: 4 [27000/50000 (54%)]\tLoss: 1795.644063\n",
      "Train Epoch: 4 [28000/50000 (56%)]\tLoss: 1800.171875\n",
      "Train Epoch: 4 [29000/50000 (58%)]\tLoss: 1816.235469\n",
      "Train Epoch: 4 [30000/50000 (60%)]\tLoss: 1826.555938\n",
      "Train Epoch: 4 [31000/50000 (62%)]\tLoss: 1820.197187\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1867.417187\n",
      "Train Epoch: 4 [33000/50000 (66%)]\tLoss: 1793.570625\n",
      "Train Epoch: 4 [34000/50000 (68%)]\tLoss: 1772.662344\n",
      "Train Epoch: 4 [35000/50000 (70%)]\tLoss: 1817.133594\n",
      "Train Epoch: 4 [36000/50000 (72%)]\tLoss: 1787.049531\n",
      "Train Epoch: 4 [37000/50000 (74%)]\tLoss: 1806.094219\n",
      "Train Epoch: 4 [38000/50000 (76%)]\tLoss: 1824.252031\n",
      "Train Epoch: 4 [39000/50000 (78%)]\tLoss: 1833.302031\n",
      "Train Epoch: 4 [40000/50000 (80%)]\tLoss: 1763.850625\n",
      "Train Epoch: 4 [41000/50000 (82%)]\tLoss: 1770.426719\n",
      "Train Epoch: 4 [42000/50000 (84%)]\tLoss: 1799.302031\n",
      "Train Epoch: 4 [43000/50000 (86%)]\tLoss: 1774.797969\n",
      "Train Epoch: 4 [44000/50000 (88%)]\tLoss: 1822.073437\n",
      "Train Epoch: 4 [45000/50000 (90%)]\tLoss: 1788.879375\n",
      "Train Epoch: 4 [46000/50000 (92%)]\tLoss: 1775.767031\n",
      "Train Epoch: 4 [47000/50000 (94%)]\tLoss: 1807.696406\n",
      "Train Epoch: 4 [48000/50000 (96%)]\tLoss: 1728.838125\n",
      "Train Epoch: 4 [49000/50000 (98%)]\tLoss: 1787.922969\n",
      "====> Epoch: 4 Average loss: 1802.5891\n",
      "\n",
      "Test set: Average loss: 1791.8567\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1739.966094\n",
      "Train Epoch: 5 [1000/50000 (2%)]\tLoss: 1758.630000\n",
      "Train Epoch: 5 [2000/50000 (4%)]\tLoss: 1792.792656\n",
      "Train Epoch: 5 [3000/50000 (6%)]\tLoss: 1738.063125\n",
      "Train Epoch: 5 [4000/50000 (8%)]\tLoss: 1768.409219\n",
      "Train Epoch: 5 [5000/50000 (10%)]\tLoss: 1804.999375\n",
      "Train Epoch: 5 [6000/50000 (12%)]\tLoss: 1779.813906\n",
      "Train Epoch: 5 [7000/50000 (14%)]\tLoss: 1862.894687\n",
      "Train Epoch: 5 [8000/50000 (16%)]\tLoss: 1791.980781\n",
      "Train Epoch: 5 [9000/50000 (18%)]\tLoss: 1776.729375\n",
      "Train Epoch: 5 [10000/50000 (20%)]\tLoss: 1796.824531\n",
      "Train Epoch: 5 [11000/50000 (22%)]\tLoss: 1777.596094\n",
      "Train Epoch: 5 [12000/50000 (24%)]\tLoss: 1818.693906\n",
      "Train Epoch: 5 [13000/50000 (26%)]\tLoss: 1813.592813\n",
      "Train Epoch: 5 [14000/50000 (28%)]\tLoss: 1771.390625\n",
      "Train Epoch: 5 [15000/50000 (30%)]\tLoss: 1790.847969\n",
      "Train Epoch: 5 [16000/50000 (32%)]\tLoss: 1792.306094\n",
      "Train Epoch: 5 [17000/50000 (34%)]\tLoss: 1806.763125\n",
      "Train Epoch: 5 [18000/50000 (36%)]\tLoss: 1801.307500\n",
      "Train Epoch: 5 [19000/50000 (38%)]\tLoss: 1752.166875\n",
      "Train Epoch: 5 [20000/50000 (40%)]\tLoss: 1790.958125\n",
      "Train Epoch: 5 [21000/50000 (42%)]\tLoss: 1798.401250\n",
      "Train Epoch: 5 [22000/50000 (44%)]\tLoss: 1769.315781\n",
      "Train Epoch: 5 [23000/50000 (46%)]\tLoss: 1785.880000\n",
      "Train Epoch: 5 [24000/50000 (48%)]\tLoss: 1808.008750\n",
      "Train Epoch: 5 [25000/50000 (50%)]\tLoss: 1795.465938\n",
      "Train Epoch: 5 [26000/50000 (52%)]\tLoss: 1775.829844\n",
      "Train Epoch: 5 [27000/50000 (54%)]\tLoss: 1809.270937\n",
      "Train Epoch: 5 [28000/50000 (56%)]\tLoss: 1777.877031\n",
      "Train Epoch: 5 [29000/50000 (58%)]\tLoss: 1773.299531\n",
      "Train Epoch: 5 [30000/50000 (60%)]\tLoss: 1779.851719\n",
      "Train Epoch: 5 [31000/50000 (62%)]\tLoss: 1828.842344\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1795.051563\n",
      "Train Epoch: 5 [33000/50000 (66%)]\tLoss: 1757.133594\n",
      "Train Epoch: 5 [34000/50000 (68%)]\tLoss: 1764.641562\n",
      "Train Epoch: 5 [35000/50000 (70%)]\tLoss: 1800.150469\n",
      "Train Epoch: 5 [36000/50000 (72%)]\tLoss: 1791.846250\n",
      "Train Epoch: 5 [37000/50000 (74%)]\tLoss: 1768.263438\n",
      "Train Epoch: 5 [38000/50000 (76%)]\tLoss: 1818.685937\n",
      "Train Epoch: 5 [39000/50000 (78%)]\tLoss: 1801.992500\n",
      "Train Epoch: 5 [40000/50000 (80%)]\tLoss: 1746.417656\n",
      "Train Epoch: 5 [41000/50000 (82%)]\tLoss: 1755.023594\n",
      "Train Epoch: 5 [42000/50000 (84%)]\tLoss: 1791.234531\n",
      "Train Epoch: 5 [43000/50000 (86%)]\tLoss: 1767.384844\n",
      "Train Epoch: 5 [44000/50000 (88%)]\tLoss: 1791.827812\n",
      "Train Epoch: 5 [45000/50000 (90%)]\tLoss: 1795.091250\n",
      "Train Epoch: 5 [46000/50000 (92%)]\tLoss: 1785.417500\n",
      "Train Epoch: 5 [47000/50000 (94%)]\tLoss: 1766.394844\n",
      "Train Epoch: 5 [48000/50000 (96%)]\tLoss: 1812.958281\n",
      "Train Epoch: 5 [49000/50000 (98%)]\tLoss: 1721.745312\n",
      "====> Epoch: 5 Average loss: 1789.3236\n",
      "\n",
      "Test set: Average loss: 1779.2616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "main4(load_old=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 1774.465156\n",
      "Train Epoch: 12 [1000/50000 (2%)]\tLoss: 1844.506719\n",
      "Train Epoch: 12 [2000/50000 (4%)]\tLoss: 1787.391875\n",
      "Train Epoch: 12 [3000/50000 (6%)]\tLoss: 1758.691094\n",
      "Train Epoch: 12 [4000/50000 (8%)]\tLoss: 1753.057969\n",
      "Train Epoch: 12 [5000/50000 (10%)]\tLoss: 1837.680469\n",
      "Train Epoch: 12 [6000/50000 (12%)]\tLoss: 1743.003281\n",
      "Train Epoch: 12 [7000/50000 (14%)]\tLoss: 1755.991406\n",
      "Train Epoch: 12 [8000/50000 (16%)]\tLoss: 1824.166875\n",
      "Train Epoch: 12 [9000/50000 (18%)]\tLoss: 1769.666406\n",
      "Train Epoch: 12 [10000/50000 (20%)]\tLoss: 1798.061094\n",
      "Train Epoch: 12 [11000/50000 (22%)]\tLoss: 1807.378594\n",
      "Train Epoch: 12 [12000/50000 (24%)]\tLoss: 1747.526406\n",
      "Train Epoch: 12 [13000/50000 (26%)]\tLoss: 1766.119844\n",
      "Train Epoch: 12 [14000/50000 (28%)]\tLoss: 1735.405937\n",
      "Train Epoch: 12 [15000/50000 (30%)]\tLoss: 1738.305469\n",
      "Train Epoch: 12 [16000/50000 (32%)]\tLoss: 1749.373281\n",
      "Train Epoch: 12 [17000/50000 (34%)]\tLoss: 1785.540313\n",
      "Train Epoch: 12 [18000/50000 (36%)]\tLoss: 1744.307344\n",
      "Train Epoch: 12 [19000/50000 (38%)]\tLoss: 1797.147344\n",
      "Train Epoch: 12 [20000/50000 (40%)]\tLoss: 1766.489375\n",
      "Train Epoch: 12 [21000/50000 (42%)]\tLoss: 1747.983594\n",
      "Train Epoch: 12 [22000/50000 (44%)]\tLoss: 1765.732812\n",
      "Train Epoch: 12 [23000/50000 (46%)]\tLoss: 1752.274688\n",
      "Train Epoch: 12 [24000/50000 (48%)]\tLoss: 1798.525938\n",
      "Train Epoch: 12 [25000/50000 (50%)]\tLoss: 1757.094219\n",
      "Train Epoch: 12 [26000/50000 (52%)]\tLoss: 1743.490625\n",
      "Train Epoch: 12 [27000/50000 (54%)]\tLoss: 1805.344219\n",
      "Train Epoch: 12 [28000/50000 (56%)]\tLoss: 1752.265000\n",
      "Train Epoch: 12 [29000/50000 (58%)]\tLoss: 1775.303437\n",
      "Train Epoch: 12 [30000/50000 (60%)]\tLoss: 1780.269531\n",
      "Train Epoch: 12 [31000/50000 (62%)]\tLoss: 1757.041875\n",
      "Train Epoch: 12 [32000/50000 (64%)]\tLoss: 1810.147344\n",
      "Train Epoch: 12 [33000/50000 (66%)]\tLoss: 1791.455469\n",
      "Train Epoch: 12 [34000/50000 (68%)]\tLoss: 1745.459844\n",
      "Train Epoch: 12 [35000/50000 (70%)]\tLoss: 1794.298125\n",
      "Train Epoch: 12 [36000/50000 (72%)]\tLoss: 1766.424375\n",
      "Train Epoch: 12 [37000/50000 (74%)]\tLoss: 1743.318750\n",
      "Train Epoch: 12 [38000/50000 (76%)]\tLoss: 1768.732500\n",
      "Train Epoch: 12 [39000/50000 (78%)]\tLoss: 1720.209687\n",
      "Train Epoch: 12 [40000/50000 (80%)]\tLoss: 1797.640312\n",
      "Train Epoch: 12 [41000/50000 (82%)]\tLoss: 1760.201562\n",
      "Train Epoch: 12 [42000/50000 (84%)]\tLoss: 1744.651719\n",
      "Train Epoch: 12 [43000/50000 (86%)]\tLoss: 1745.875625\n",
      "Train Epoch: 12 [44000/50000 (88%)]\tLoss: 1793.920469\n",
      "Train Epoch: 12 [45000/50000 (90%)]\tLoss: 1801.933281\n",
      "Train Epoch: 12 [46000/50000 (92%)]\tLoss: 1793.006094\n",
      "Train Epoch: 12 [47000/50000 (94%)]\tLoss: 1780.831563\n",
      "Train Epoch: 12 [48000/50000 (96%)]\tLoss: 1801.308438\n",
      "Train Epoch: 12 [49000/50000 (98%)]\tLoss: 1778.394219\n",
      "====> Epoch: 12 Average loss: 1777.1562\n",
      "\n",
      "Test set: Average loss: 1766.0156\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 1788.285312\n",
      "Train Epoch: 13 [1000/50000 (2%)]\tLoss: 1732.731406\n",
      "Train Epoch: 13 [2000/50000 (4%)]\tLoss: 1784.680938\n",
      "Train Epoch: 13 [3000/50000 (6%)]\tLoss: 1741.875781\n",
      "Train Epoch: 13 [4000/50000 (8%)]\tLoss: 1741.230625\n",
      "Train Epoch: 13 [5000/50000 (10%)]\tLoss: 1768.082969\n",
      "Train Epoch: 13 [6000/50000 (12%)]\tLoss: 1793.219844\n",
      "Train Epoch: 13 [7000/50000 (14%)]\tLoss: 1739.393437\n",
      "Train Epoch: 13 [8000/50000 (16%)]\tLoss: 1768.973594\n",
      "Train Epoch: 13 [9000/50000 (18%)]\tLoss: 1759.017500\n",
      "Train Epoch: 13 [10000/50000 (20%)]\tLoss: 1766.048750\n",
      "Train Epoch: 13 [11000/50000 (22%)]\tLoss: 1821.239063\n",
      "Train Epoch: 13 [12000/50000 (24%)]\tLoss: 1769.901250\n",
      "Train Epoch: 13 [13000/50000 (26%)]\tLoss: 1780.557344\n",
      "Train Epoch: 13 [14000/50000 (28%)]\tLoss: 1721.346562\n",
      "Train Epoch: 13 [15000/50000 (30%)]\tLoss: 1771.747344\n",
      "Train Epoch: 13 [16000/50000 (32%)]\tLoss: 1778.386250\n",
      "Train Epoch: 13 [17000/50000 (34%)]\tLoss: 1763.655625\n",
      "Train Epoch: 13 [18000/50000 (36%)]\tLoss: 1768.501719\n",
      "Train Epoch: 13 [19000/50000 (38%)]\tLoss: 1743.690312\n",
      "Train Epoch: 13 [20000/50000 (40%)]\tLoss: 1777.529063\n",
      "Train Epoch: 13 [21000/50000 (42%)]\tLoss: 1716.473750\n",
      "Train Epoch: 13 [22000/50000 (44%)]\tLoss: 1774.238437\n",
      "Train Epoch: 13 [23000/50000 (46%)]\tLoss: 1796.200156\n",
      "Train Epoch: 13 [24000/50000 (48%)]\tLoss: 1776.047344\n",
      "Train Epoch: 13 [25000/50000 (50%)]\tLoss: 1773.002500\n",
      "Train Epoch: 13 [26000/50000 (52%)]\tLoss: 1790.446719\n",
      "Train Epoch: 13 [27000/50000 (54%)]\tLoss: 1726.033281\n",
      "Train Epoch: 13 [28000/50000 (56%)]\tLoss: 1778.605937\n",
      "Train Epoch: 13 [29000/50000 (58%)]\tLoss: 1780.576719\n",
      "Train Epoch: 13 [30000/50000 (60%)]\tLoss: 1779.775156\n",
      "Train Epoch: 13 [31000/50000 (62%)]\tLoss: 1783.724688\n",
      "Train Epoch: 13 [32000/50000 (64%)]\tLoss: 1766.781875\n",
      "Train Epoch: 13 [33000/50000 (66%)]\tLoss: 1702.622500\n",
      "Train Epoch: 13 [34000/50000 (68%)]\tLoss: 1769.889219\n",
      "Train Epoch: 13 [35000/50000 (70%)]\tLoss: 1806.523750\n",
      "Train Epoch: 13 [36000/50000 (72%)]\tLoss: 1735.926406\n",
      "Train Epoch: 13 [37000/50000 (74%)]\tLoss: 1798.735156\n",
      "Train Epoch: 13 [38000/50000 (76%)]\tLoss: 1809.258594\n",
      "Train Epoch: 13 [39000/50000 (78%)]\tLoss: 1720.857188\n",
      "Train Epoch: 13 [40000/50000 (80%)]\tLoss: 1762.550000\n",
      "Train Epoch: 13 [41000/50000 (82%)]\tLoss: 1780.913438\n",
      "Train Epoch: 13 [42000/50000 (84%)]\tLoss: 1745.043594\n",
      "Train Epoch: 13 [43000/50000 (86%)]\tLoss: 1770.607500\n",
      "Train Epoch: 13 [44000/50000 (88%)]\tLoss: 1788.631562\n",
      "Train Epoch: 13 [45000/50000 (90%)]\tLoss: 1764.154687\n",
      "Train Epoch: 13 [46000/50000 (92%)]\tLoss: 1733.407031\n",
      "Train Epoch: 13 [47000/50000 (94%)]\tLoss: 1768.656250\n",
      "Train Epoch: 13 [48000/50000 (96%)]\tLoss: 1745.278125\n",
      "Train Epoch: 13 [49000/50000 (98%)]\tLoss: 1712.654844\n",
      "====> Epoch: 13 Average loss: 1766.1124\n",
      "\n",
      "Test set: Average loss: 1768.3704\n",
      "\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 1757.509063\n",
      "Train Epoch: 14 [1000/50000 (2%)]\tLoss: 1758.021563\n",
      "Train Epoch: 14 [2000/50000 (4%)]\tLoss: 1750.550313\n",
      "Train Epoch: 14 [3000/50000 (6%)]\tLoss: 1749.460313\n",
      "Train Epoch: 14 [4000/50000 (8%)]\tLoss: 1776.909844\n",
      "Train Epoch: 14 [5000/50000 (10%)]\tLoss: 1745.958906\n",
      "Train Epoch: 14 [6000/50000 (12%)]\tLoss: 1791.814687\n",
      "Train Epoch: 14 [7000/50000 (14%)]\tLoss: 1763.538750\n",
      "Train Epoch: 14 [8000/50000 (16%)]\tLoss: 1737.936719\n",
      "Train Epoch: 14 [9000/50000 (18%)]\tLoss: 1783.834531\n",
      "Train Epoch: 14 [10000/50000 (20%)]\tLoss: 1728.344687\n",
      "Train Epoch: 14 [11000/50000 (22%)]\tLoss: 1762.909844\n",
      "Train Epoch: 14 [12000/50000 (24%)]\tLoss: 1778.887187\n",
      "Train Epoch: 14 [13000/50000 (26%)]\tLoss: 1775.679063\n",
      "Train Epoch: 14 [14000/50000 (28%)]\tLoss: 1784.029219\n",
      "Train Epoch: 14 [15000/50000 (30%)]\tLoss: 1761.666563\n",
      "Train Epoch: 14 [16000/50000 (32%)]\tLoss: 1767.788281\n",
      "Train Epoch: 14 [17000/50000 (34%)]\tLoss: 1793.977656\n",
      "Train Epoch: 14 [18000/50000 (36%)]\tLoss: 1789.462188\n",
      "Train Epoch: 14 [19000/50000 (38%)]\tLoss: 1780.310469\n",
      "Train Epoch: 14 [20000/50000 (40%)]\tLoss: 1759.970156\n",
      "Train Epoch: 14 [21000/50000 (42%)]\tLoss: 1734.656563\n",
      "Train Epoch: 14 [22000/50000 (44%)]\tLoss: 1801.430469\n",
      "Train Epoch: 14 [23000/50000 (46%)]\tLoss: 1778.705156\n",
      "Train Epoch: 14 [24000/50000 (48%)]\tLoss: 1726.612656\n",
      "Train Epoch: 14 [25000/50000 (50%)]\tLoss: 1782.897344\n",
      "Train Epoch: 14 [26000/50000 (52%)]\tLoss: 1791.154687\n",
      "Train Epoch: 14 [27000/50000 (54%)]\tLoss: 1770.278594\n",
      "Train Epoch: 14 [28000/50000 (56%)]\tLoss: 1764.076562\n",
      "Train Epoch: 14 [29000/50000 (58%)]\tLoss: 1761.426094\n",
      "Train Epoch: 14 [30000/50000 (60%)]\tLoss: 1778.802344\n",
      "Train Epoch: 14 [31000/50000 (62%)]\tLoss: 1817.276250\n",
      "Train Epoch: 14 [32000/50000 (64%)]\tLoss: 1721.191562\n",
      "Train Epoch: 14 [33000/50000 (66%)]\tLoss: 1733.538281\n",
      "Train Epoch: 14 [34000/50000 (68%)]\tLoss: 1742.189844\n",
      "Train Epoch: 14 [35000/50000 (70%)]\tLoss: 1770.000313\n",
      "Train Epoch: 14 [36000/50000 (72%)]\tLoss: 1771.304844\n",
      "Train Epoch: 14 [37000/50000 (74%)]\tLoss: 1794.223594\n",
      "Train Epoch: 14 [38000/50000 (76%)]\tLoss: 1752.813437\n",
      "Train Epoch: 14 [39000/50000 (78%)]\tLoss: 1780.539531\n",
      "Train Epoch: 14 [40000/50000 (80%)]\tLoss: 1776.545781\n",
      "Train Epoch: 14 [41000/50000 (82%)]\tLoss: 1774.740000\n",
      "Train Epoch: 14 [42000/50000 (84%)]\tLoss: 1715.490000\n",
      "Train Epoch: 14 [43000/50000 (86%)]\tLoss: 1761.989375\n",
      "Train Epoch: 14 [44000/50000 (88%)]\tLoss: 1769.076719\n",
      "Train Epoch: 14 [45000/50000 (90%)]\tLoss: 1769.300781\n",
      "Train Epoch: 14 [46000/50000 (92%)]\tLoss: 1770.666563\n",
      "Train Epoch: 14 [47000/50000 (94%)]\tLoss: 1754.731875\n",
      "Train Epoch: 14 [48000/50000 (96%)]\tLoss: 1734.600469\n",
      "Train Epoch: 14 [49000/50000 (98%)]\tLoss: 1782.980469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 14 Average loss: 1764.9209\n",
      "\n",
      "Test set: Average loss: 1763.2856\n",
      "\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 1765.659375\n",
      "Train Epoch: 15 [1000/50000 (2%)]\tLoss: 1735.243125\n",
      "Train Epoch: 15 [2000/50000 (4%)]\tLoss: 1767.027969\n",
      "Train Epoch: 15 [3000/50000 (6%)]\tLoss: 1766.319844\n",
      "Train Epoch: 15 [4000/50000 (8%)]\tLoss: 1776.606094\n",
      "Train Epoch: 15 [5000/50000 (10%)]\tLoss: 1756.287812\n",
      "Train Epoch: 15 [6000/50000 (12%)]\tLoss: 1742.349844\n",
      "Train Epoch: 15 [7000/50000 (14%)]\tLoss: 1783.806406\n",
      "Train Epoch: 15 [8000/50000 (16%)]\tLoss: 1755.215781\n",
      "Train Epoch: 15 [9000/50000 (18%)]\tLoss: 1762.606250\n",
      "Train Epoch: 15 [10000/50000 (20%)]\tLoss: 1790.767656\n",
      "Train Epoch: 15 [11000/50000 (22%)]\tLoss: 1791.471562\n",
      "Train Epoch: 15 [12000/50000 (24%)]\tLoss: 1792.787344\n",
      "Train Epoch: 15 [13000/50000 (26%)]\tLoss: 1729.654844\n",
      "Train Epoch: 15 [14000/50000 (28%)]\tLoss: 1755.251406\n",
      "Train Epoch: 15 [15000/50000 (30%)]\tLoss: 1766.200000\n",
      "Train Epoch: 15 [16000/50000 (32%)]\tLoss: 1763.418437\n",
      "Train Epoch: 15 [17000/50000 (34%)]\tLoss: 1762.165625\n",
      "Train Epoch: 15 [18000/50000 (36%)]\tLoss: 1743.314687\n",
      "Train Epoch: 15 [19000/50000 (38%)]\tLoss: 1751.759375\n",
      "Train Epoch: 15 [20000/50000 (40%)]\tLoss: 1748.462969\n",
      "Train Epoch: 15 [21000/50000 (42%)]\tLoss: 1720.583281\n",
      "Train Epoch: 15 [22000/50000 (44%)]\tLoss: 1748.232812\n",
      "Train Epoch: 15 [23000/50000 (46%)]\tLoss: 1752.830312\n",
      "Train Epoch: 15 [24000/50000 (48%)]\tLoss: 1766.927969\n",
      "Train Epoch: 15 [25000/50000 (50%)]\tLoss: 1744.285781\n",
      "Train Epoch: 15 [26000/50000 (52%)]\tLoss: 1706.743594\n",
      "Train Epoch: 15 [27000/50000 (54%)]\tLoss: 1758.975938\n",
      "Train Epoch: 15 [28000/50000 (56%)]\tLoss: 1773.822813\n",
      "Train Epoch: 15 [29000/50000 (58%)]\tLoss: 1786.790000\n",
      "Train Epoch: 15 [30000/50000 (60%)]\tLoss: 1784.712812\n",
      "Train Epoch: 15 [31000/50000 (62%)]\tLoss: 1784.258750\n",
      "Train Epoch: 15 [32000/50000 (64%)]\tLoss: 1835.286875\n",
      "Train Epoch: 15 [33000/50000 (66%)]\tLoss: 1753.036406\n",
      "Train Epoch: 15 [34000/50000 (68%)]\tLoss: 1740.658750\n",
      "Train Epoch: 15 [35000/50000 (70%)]\tLoss: 1783.392812\n",
      "Train Epoch: 15 [36000/50000 (72%)]\tLoss: 1760.422500\n",
      "Train Epoch: 15 [37000/50000 (74%)]\tLoss: 1752.593906\n",
      "Train Epoch: 15 [38000/50000 (76%)]\tLoss: 1808.619219\n",
      "Train Epoch: 15 [39000/50000 (78%)]\tLoss: 1799.554063\n",
      "Train Epoch: 15 [40000/50000 (80%)]\tLoss: 1727.642188\n",
      "Train Epoch: 15 [41000/50000 (82%)]\tLoss: 1743.539531\n",
      "Train Epoch: 15 [42000/50000 (84%)]\tLoss: 1769.570469\n",
      "Train Epoch: 15 [43000/50000 (86%)]\tLoss: 1735.717656\n",
      "Train Epoch: 15 [44000/50000 (88%)]\tLoss: 1788.175000\n",
      "Train Epoch: 15 [45000/50000 (90%)]\tLoss: 1737.845469\n",
      "Train Epoch: 15 [46000/50000 (92%)]\tLoss: 1746.875937\n",
      "Train Epoch: 15 [47000/50000 (94%)]\tLoss: 1760.600781\n",
      "Train Epoch: 15 [48000/50000 (96%)]\tLoss: 1703.540469\n",
      "Train Epoch: 15 [49000/50000 (98%)]\tLoss: 1754.054375\n",
      "====> Epoch: 15 Average loss: 1764.1853\n",
      "\n",
      "Test set: Average loss: 1775.1307\n",
      "\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 1721.525625\n",
      "Train Epoch: 16 [1000/50000 (2%)]\tLoss: 1730.559688\n",
      "Train Epoch: 16 [2000/50000 (4%)]\tLoss: 1758.286875\n",
      "Train Epoch: 16 [3000/50000 (6%)]\tLoss: 1713.444062\n",
      "Train Epoch: 16 [4000/50000 (8%)]\tLoss: 1749.110625\n",
      "Train Epoch: 16 [5000/50000 (10%)]\tLoss: 1755.582344\n",
      "Train Epoch: 16 [6000/50000 (12%)]\tLoss: 1754.178281\n",
      "Train Epoch: 16 [7000/50000 (14%)]\tLoss: 1825.833906\n",
      "Train Epoch: 16 [8000/50000 (16%)]\tLoss: 1753.021406\n",
      "Train Epoch: 16 [9000/50000 (18%)]\tLoss: 1745.592656\n",
      "Train Epoch: 16 [10000/50000 (20%)]\tLoss: 1772.790937\n",
      "Train Epoch: 16 [11000/50000 (22%)]\tLoss: 1753.465938\n",
      "Train Epoch: 16 [12000/50000 (24%)]\tLoss: 1799.049531\n",
      "Train Epoch: 16 [13000/50000 (26%)]\tLoss: 1772.207813\n",
      "Train Epoch: 16 [14000/50000 (28%)]\tLoss: 1752.570469\n",
      "Train Epoch: 16 [15000/50000 (30%)]\tLoss: 1762.325469\n",
      "Train Epoch: 16 [16000/50000 (32%)]\tLoss: 1768.682969\n",
      "Train Epoch: 16 [17000/50000 (34%)]\tLoss: 1784.101094\n",
      "Train Epoch: 16 [18000/50000 (36%)]\tLoss: 1763.176406\n",
      "Train Epoch: 16 [19000/50000 (38%)]\tLoss: 1717.163438\n",
      "Train Epoch: 16 [20000/50000 (40%)]\tLoss: 1765.189375\n",
      "Train Epoch: 16 [21000/50000 (42%)]\tLoss: 1772.294844\n",
      "Train Epoch: 16 [22000/50000 (44%)]\tLoss: 1746.306250\n",
      "Train Epoch: 16 [23000/50000 (46%)]\tLoss: 1743.468906\n",
      "Train Epoch: 16 [24000/50000 (48%)]\tLoss: 1783.163281\n",
      "Train Epoch: 16 [25000/50000 (50%)]\tLoss: 1772.348750\n",
      "Train Epoch: 16 [26000/50000 (52%)]\tLoss: 1744.700156\n",
      "Train Epoch: 16 [27000/50000 (54%)]\tLoss: 1794.897656\n",
      "Train Epoch: 16 [28000/50000 (56%)]\tLoss: 1764.165469\n",
      "Train Epoch: 16 [29000/50000 (58%)]\tLoss: 1756.512969\n",
      "Train Epoch: 16 [30000/50000 (60%)]\tLoss: 1761.507500\n",
      "Train Epoch: 16 [31000/50000 (62%)]\tLoss: 1808.662500\n",
      "Train Epoch: 16 [32000/50000 (64%)]\tLoss: 1771.124531\n",
      "Train Epoch: 16 [33000/50000 (66%)]\tLoss: 1737.475000\n",
      "Train Epoch: 16 [34000/50000 (68%)]\tLoss: 1743.072656\n",
      "Train Epoch: 16 [35000/50000 (70%)]\tLoss: 1777.200313\n",
      "Train Epoch: 16 [36000/50000 (72%)]\tLoss: 1789.426250\n",
      "Train Epoch: 16 [37000/50000 (74%)]\tLoss: 1737.850781\n",
      "Train Epoch: 16 [38000/50000 (76%)]\tLoss: 1795.912344\n",
      "Train Epoch: 16 [39000/50000 (78%)]\tLoss: 1766.272969\n",
      "Train Epoch: 16 [40000/50000 (80%)]\tLoss: 1723.930156\n",
      "Train Epoch: 16 [41000/50000 (82%)]\tLoss: 1731.481719\n",
      "Train Epoch: 16 [42000/50000 (84%)]\tLoss: 1773.739063\n",
      "Train Epoch: 16 [43000/50000 (86%)]\tLoss: 1745.203906\n",
      "Train Epoch: 16 [44000/50000 (88%)]\tLoss: 1776.499219\n",
      "Train Epoch: 16 [45000/50000 (90%)]\tLoss: 1774.701719\n",
      "Train Epoch: 16 [46000/50000 (92%)]\tLoss: 1770.018906\n",
      "Train Epoch: 16 [47000/50000 (94%)]\tLoss: 1740.403125\n",
      "Train Epoch: 16 [48000/50000 (96%)]\tLoss: 1783.898906\n",
      "Train Epoch: 16 [49000/50000 (98%)]\tLoss: 1699.694062\n",
      "====> Epoch: 16 Average loss: 1763.4564\n",
      "\n",
      "Test set: Average loss: 1765.7368\n",
      "\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 1730.994062\n",
      "Train Epoch: 17 [1000/50000 (2%)]\tLoss: 1720.188594\n",
      "Train Epoch: 17 [2000/50000 (4%)]\tLoss: 1781.027031\n",
      "Train Epoch: 17 [3000/50000 (6%)]\tLoss: 1753.511250\n",
      "Train Epoch: 17 [4000/50000 (8%)]\tLoss: 1737.297500\n",
      "Train Epoch: 17 [5000/50000 (10%)]\tLoss: 1763.649531\n",
      "Train Epoch: 17 [6000/50000 (12%)]\tLoss: 1763.563594\n",
      "Train Epoch: 17 [7000/50000 (14%)]\tLoss: 1813.977187\n",
      "Train Epoch: 17 [8000/50000 (16%)]\tLoss: 1774.265312\n",
      "Train Epoch: 17 [9000/50000 (18%)]\tLoss: 1765.236875\n",
      "Train Epoch: 17 [10000/50000 (20%)]\tLoss: 1741.736250\n",
      "Train Epoch: 17 [11000/50000 (22%)]\tLoss: 1735.100625\n",
      "Train Epoch: 17 [12000/50000 (24%)]\tLoss: 1746.371562\n",
      "Train Epoch: 17 [13000/50000 (26%)]\tLoss: 1768.385156\n",
      "Train Epoch: 17 [14000/50000 (28%)]\tLoss: 1736.627812\n",
      "Train Epoch: 17 [15000/50000 (30%)]\tLoss: 1724.236562\n",
      "Train Epoch: 17 [16000/50000 (32%)]\tLoss: 1768.331094\n",
      "Train Epoch: 17 [17000/50000 (34%)]\tLoss: 1783.064687\n",
      "Train Epoch: 17 [18000/50000 (36%)]\tLoss: 1737.070156\n",
      "Train Epoch: 17 [19000/50000 (38%)]\tLoss: 1745.067031\n",
      "Train Epoch: 17 [20000/50000 (40%)]\tLoss: 1769.602656\n",
      "Train Epoch: 17 [21000/50000 (42%)]\tLoss: 1768.330625\n",
      "Train Epoch: 17 [22000/50000 (44%)]\tLoss: 1733.407187\n",
      "Train Epoch: 17 [23000/50000 (46%)]\tLoss: 1795.112031\n",
      "Train Epoch: 17 [24000/50000 (48%)]\tLoss: 1708.205625\n",
      "Train Epoch: 17 [25000/50000 (50%)]\tLoss: 1765.858281\n",
      "Train Epoch: 17 [26000/50000 (52%)]\tLoss: 1788.432812\n",
      "Train Epoch: 17 [27000/50000 (54%)]\tLoss: 1766.442031\n",
      "Train Epoch: 17 [28000/50000 (56%)]\tLoss: 1810.475312\n",
      "Train Epoch: 17 [29000/50000 (58%)]\tLoss: 1759.456406\n",
      "Train Epoch: 17 [30000/50000 (60%)]\tLoss: 1761.153281\n",
      "Train Epoch: 17 [31000/50000 (62%)]\tLoss: 1725.390156\n",
      "Train Epoch: 17 [32000/50000 (64%)]\tLoss: 1767.192188\n",
      "Train Epoch: 17 [33000/50000 (66%)]\tLoss: 1787.378125\n",
      "Train Epoch: 17 [34000/50000 (68%)]\tLoss: 1769.371719\n",
      "Train Epoch: 17 [35000/50000 (70%)]\tLoss: 1743.939063\n",
      "Train Epoch: 17 [36000/50000 (72%)]\tLoss: 1745.023750\n",
      "Train Epoch: 17 [37000/50000 (74%)]\tLoss: 1766.614375\n",
      "Train Epoch: 17 [38000/50000 (76%)]\tLoss: 1802.632812\n",
      "Train Epoch: 17 [39000/50000 (78%)]\tLoss: 1699.810156\n",
      "Train Epoch: 17 [40000/50000 (80%)]\tLoss: 1747.079531\n",
      "Train Epoch: 17 [41000/50000 (82%)]\tLoss: 1801.601719\n",
      "Train Epoch: 17 [42000/50000 (84%)]\tLoss: 1755.332344\n",
      "Train Epoch: 17 [43000/50000 (86%)]\tLoss: 1750.492969\n",
      "Train Epoch: 17 [44000/50000 (88%)]\tLoss: 1707.048750\n",
      "Train Epoch: 17 [45000/50000 (90%)]\tLoss: 1788.370938\n",
      "Train Epoch: 17 [46000/50000 (92%)]\tLoss: 1754.968906\n",
      "Train Epoch: 17 [47000/50000 (94%)]\tLoss: 1727.968125\n",
      "Train Epoch: 17 [48000/50000 (96%)]\tLoss: 1772.271406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 17 [49000/50000 (98%)]\tLoss: 1786.969375\n",
      "====> Epoch: 17 Average loss: 1762.7752\n",
      "\n",
      "Test set: Average loss: 1768.2812\n",
      "\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 1770.938906\n",
      "Train Epoch: 18 [1000/50000 (2%)]\tLoss: 1773.998906\n",
      "Train Epoch: 18 [2000/50000 (4%)]\tLoss: 1719.413906\n",
      "Train Epoch: 18 [3000/50000 (6%)]\tLoss: 1823.063594\n",
      "Train Epoch: 18 [4000/50000 (8%)]\tLoss: 1737.874063\n",
      "Train Epoch: 18 [5000/50000 (10%)]\tLoss: 1776.918906\n",
      "Train Epoch: 18 [6000/50000 (12%)]\tLoss: 1735.944688\n",
      "Train Epoch: 18 [7000/50000 (14%)]\tLoss: 1755.572656\n",
      "Train Epoch: 18 [8000/50000 (16%)]\tLoss: 1732.528594\n",
      "Train Epoch: 18 [9000/50000 (18%)]\tLoss: 1799.856250\n",
      "Train Epoch: 18 [10000/50000 (20%)]\tLoss: 1709.859531\n",
      "Train Epoch: 18 [11000/50000 (22%)]\tLoss: 1733.620938\n",
      "Train Epoch: 18 [12000/50000 (24%)]\tLoss: 1760.305781\n",
      "Train Epoch: 18 [13000/50000 (26%)]\tLoss: 1777.393125\n",
      "Train Epoch: 18 [14000/50000 (28%)]\tLoss: 1752.857812\n",
      "Train Epoch: 18 [15000/50000 (30%)]\tLoss: 1809.843281\n",
      "Train Epoch: 18 [16000/50000 (32%)]\tLoss: 1721.466250\n",
      "Train Epoch: 18 [17000/50000 (34%)]\tLoss: 1792.482344\n",
      "Train Epoch: 18 [18000/50000 (36%)]\tLoss: 1746.913281\n",
      "Train Epoch: 18 [19000/50000 (38%)]\tLoss: 1756.280781\n",
      "Train Epoch: 18 [20000/50000 (40%)]\tLoss: 1773.941719\n",
      "Train Epoch: 18 [21000/50000 (42%)]\tLoss: 1743.684844\n",
      "Train Epoch: 18 [22000/50000 (44%)]\tLoss: 1754.448281\n",
      "Train Epoch: 18 [23000/50000 (46%)]\tLoss: 1784.280000\n",
      "Train Epoch: 18 [24000/50000 (48%)]\tLoss: 1760.523594\n",
      "Train Epoch: 18 [25000/50000 (50%)]\tLoss: 1740.383750\n",
      "Train Epoch: 18 [26000/50000 (52%)]\tLoss: 1762.968281\n",
      "Train Epoch: 18 [27000/50000 (54%)]\tLoss: 1744.616875\n",
      "Train Epoch: 18 [28000/50000 (56%)]\tLoss: 1752.459063\n",
      "Train Epoch: 18 [29000/50000 (58%)]\tLoss: 1749.224844\n",
      "Train Epoch: 18 [30000/50000 (60%)]\tLoss: 1776.204844\n",
      "Train Epoch: 18 [31000/50000 (62%)]\tLoss: 1772.541094\n",
      "Train Epoch: 18 [32000/50000 (64%)]\tLoss: 1753.115937\n",
      "Train Epoch: 18 [33000/50000 (66%)]\tLoss: 1763.500313\n",
      "Train Epoch: 18 [34000/50000 (68%)]\tLoss: 1750.865000\n",
      "Train Epoch: 18 [35000/50000 (70%)]\tLoss: 1766.711719\n",
      "Train Epoch: 18 [36000/50000 (72%)]\tLoss: 1768.145781\n",
      "Train Epoch: 18 [37000/50000 (74%)]\tLoss: 1766.451406\n",
      "Train Epoch: 18 [38000/50000 (76%)]\tLoss: 1750.210469\n",
      "Train Epoch: 18 [39000/50000 (78%)]\tLoss: 1773.620469\n",
      "Train Epoch: 18 [40000/50000 (80%)]\tLoss: 1758.660312\n",
      "Train Epoch: 18 [41000/50000 (82%)]\tLoss: 1769.885781\n",
      "Train Epoch: 18 [42000/50000 (84%)]\tLoss: 1782.924219\n",
      "Train Epoch: 18 [43000/50000 (86%)]\tLoss: 1774.834844\n",
      "Train Epoch: 18 [44000/50000 (88%)]\tLoss: 1731.100000\n",
      "Train Epoch: 18 [45000/50000 (90%)]\tLoss: 1774.725312\n",
      "Train Epoch: 18 [46000/50000 (92%)]\tLoss: 1714.322187\n",
      "Train Epoch: 18 [47000/50000 (94%)]\tLoss: 1801.272813\n",
      "Train Epoch: 18 [48000/50000 (96%)]\tLoss: 1790.349375\n",
      "Train Epoch: 18 [49000/50000 (98%)]\tLoss: 1751.559062\n",
      "====> Epoch: 18 Average loss: 1762.1601\n",
      "\n",
      "Test set: Average loss: 1768.6068\n",
      "\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 1766.295312\n",
      "Train Epoch: 19 [1000/50000 (2%)]\tLoss: 1789.433281\n",
      "Train Epoch: 19 [2000/50000 (4%)]\tLoss: 1731.752969\n",
      "Train Epoch: 19 [3000/50000 (6%)]\tLoss: 1766.002812\n",
      "Train Epoch: 19 [4000/50000 (8%)]\tLoss: 1765.571094\n",
      "Train Epoch: 19 [5000/50000 (10%)]\tLoss: 1777.965312\n",
      "Train Epoch: 19 [6000/50000 (12%)]\tLoss: 1722.511406\n",
      "Train Epoch: 19 [7000/50000 (14%)]\tLoss: 1748.986094\n",
      "Train Epoch: 19 [8000/50000 (16%)]\tLoss: 1764.721094\n",
      "Train Epoch: 19 [9000/50000 (18%)]\tLoss: 1778.374375\n",
      "Train Epoch: 19 [10000/50000 (20%)]\tLoss: 1768.137344\n",
      "Train Epoch: 19 [11000/50000 (22%)]\tLoss: 1743.178281\n",
      "Train Epoch: 19 [12000/50000 (24%)]\tLoss: 1753.931250\n",
      "Train Epoch: 19 [13000/50000 (26%)]\tLoss: 1759.921875\n",
      "Train Epoch: 19 [14000/50000 (28%)]\tLoss: 1756.203281\n",
      "Train Epoch: 19 [15000/50000 (30%)]\tLoss: 1723.426875\n",
      "Train Epoch: 19 [16000/50000 (32%)]\tLoss: 1790.927656\n",
      "Train Epoch: 19 [17000/50000 (34%)]\tLoss: 1696.410469\n",
      "Train Epoch: 19 [18000/50000 (36%)]\tLoss: 1776.838750\n",
      "Train Epoch: 19 [19000/50000 (38%)]\tLoss: 1797.201406\n",
      "Train Epoch: 19 [20000/50000 (40%)]\tLoss: 1744.361094\n",
      "Train Epoch: 19 [21000/50000 (42%)]\tLoss: 1752.875000\n",
      "Train Epoch: 19 [22000/50000 (44%)]\tLoss: 1785.380938\n",
      "Train Epoch: 19 [23000/50000 (46%)]\tLoss: 1727.687031\n",
      "Train Epoch: 19 [24000/50000 (48%)]\tLoss: 1719.325938\n",
      "Train Epoch: 19 [25000/50000 (50%)]\tLoss: 1744.920469\n",
      "Train Epoch: 19 [26000/50000 (52%)]\tLoss: 1722.293125\n",
      "Train Epoch: 19 [27000/50000 (54%)]\tLoss: 1756.195000\n",
      "Train Epoch: 19 [28000/50000 (56%)]\tLoss: 1756.056406\n",
      "Train Epoch: 19 [29000/50000 (58%)]\tLoss: 1736.710781\n",
      "Train Epoch: 19 [30000/50000 (60%)]\tLoss: 1718.080937\n",
      "Train Epoch: 19 [31000/50000 (62%)]\tLoss: 1721.245938\n",
      "Train Epoch: 19 [32000/50000 (64%)]\tLoss: 1748.674531\n",
      "Train Epoch: 19 [33000/50000 (66%)]\tLoss: 1783.383281\n",
      "Train Epoch: 19 [34000/50000 (68%)]\tLoss: 1770.123750\n",
      "Train Epoch: 19 [35000/50000 (70%)]\tLoss: 1740.348594\n",
      "Train Epoch: 19 [36000/50000 (72%)]\tLoss: 1759.347188\n",
      "Train Epoch: 19 [37000/50000 (74%)]\tLoss: 1744.623906\n",
      "Train Epoch: 19 [38000/50000 (76%)]\tLoss: 1760.937187\n",
      "Train Epoch: 19 [39000/50000 (78%)]\tLoss: 1749.961875\n",
      "Train Epoch: 19 [40000/50000 (80%)]\tLoss: 1750.207813\n",
      "Train Epoch: 19 [41000/50000 (82%)]\tLoss: 1799.372656\n",
      "Train Epoch: 19 [42000/50000 (84%)]\tLoss: 1770.705937\n",
      "Train Epoch: 19 [43000/50000 (86%)]\tLoss: 1753.410312\n",
      "Train Epoch: 19 [44000/50000 (88%)]\tLoss: 1767.998750\n",
      "Train Epoch: 19 [45000/50000 (90%)]\tLoss: 1758.157344\n",
      "Train Epoch: 19 [46000/50000 (92%)]\tLoss: 1790.363906\n",
      "Train Epoch: 19 [47000/50000 (94%)]\tLoss: 1756.358125\n",
      "Train Epoch: 19 [48000/50000 (96%)]\tLoss: 1772.949844\n",
      "Train Epoch: 19 [49000/50000 (98%)]\tLoss: 1770.550625\n",
      "====> Epoch: 19 Average loss: 1761.8192\n",
      "\n",
      "Test set: Average loss: 1767.0878\n",
      "\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 1742.921250\n",
      "Train Epoch: 20 [1000/50000 (2%)]\tLoss: 1791.152656\n",
      "Train Epoch: 20 [2000/50000 (4%)]\tLoss: 1775.647969\n",
      "Train Epoch: 20 [3000/50000 (6%)]\tLoss: 1694.558906\n",
      "Train Epoch: 20 [4000/50000 (8%)]\tLoss: 1744.709219\n",
      "Train Epoch: 20 [5000/50000 (10%)]\tLoss: 1748.946406\n",
      "Train Epoch: 20 [6000/50000 (12%)]\tLoss: 1771.515156\n",
      "Train Epoch: 20 [7000/50000 (14%)]\tLoss: 1732.636406\n",
      "Train Epoch: 20 [8000/50000 (16%)]\tLoss: 1749.685156\n",
      "Train Epoch: 20 [9000/50000 (18%)]\tLoss: 1781.635625\n",
      "Train Epoch: 20 [10000/50000 (20%)]\tLoss: 1721.549687\n",
      "Train Epoch: 20 [11000/50000 (22%)]\tLoss: 1754.753594\n",
      "Train Epoch: 20 [12000/50000 (24%)]\tLoss: 1778.110938\n",
      "Train Epoch: 20 [13000/50000 (26%)]\tLoss: 1736.941406\n",
      "Train Epoch: 20 [14000/50000 (28%)]\tLoss: 1722.028594\n",
      "Train Epoch: 20 [15000/50000 (30%)]\tLoss: 1733.898438\n",
      "Train Epoch: 20 [16000/50000 (32%)]\tLoss: 1775.663125\n",
      "Train Epoch: 20 [17000/50000 (34%)]\tLoss: 1778.803906\n",
      "Train Epoch: 20 [18000/50000 (36%)]\tLoss: 1775.785469\n",
      "Train Epoch: 20 [19000/50000 (38%)]\tLoss: 1770.192656\n",
      "Train Epoch: 20 [20000/50000 (40%)]\tLoss: 1744.411562\n",
      "Train Epoch: 20 [21000/50000 (42%)]\tLoss: 1762.754844\n",
      "Train Epoch: 20 [22000/50000 (44%)]\tLoss: 1725.057344\n",
      "Train Epoch: 20 [23000/50000 (46%)]\tLoss: 1770.684844\n",
      "Train Epoch: 20 [24000/50000 (48%)]\tLoss: 1784.167500\n",
      "Train Epoch: 20 [25000/50000 (50%)]\tLoss: 1765.054219\n",
      "Train Epoch: 20 [26000/50000 (52%)]\tLoss: 1798.936875\n",
      "Train Epoch: 20 [27000/50000 (54%)]\tLoss: 1814.791719\n",
      "Train Epoch: 20 [28000/50000 (56%)]\tLoss: 1788.758594\n",
      "Train Epoch: 20 [29000/50000 (58%)]\tLoss: 1734.462656\n",
      "Train Epoch: 20 [30000/50000 (60%)]\tLoss: 1746.638906\n",
      "Train Epoch: 20 [31000/50000 (62%)]\tLoss: 1781.694375\n",
      "Train Epoch: 20 [32000/50000 (64%)]\tLoss: 1796.082187\n",
      "Train Epoch: 20 [33000/50000 (66%)]\tLoss: 1753.428281\n",
      "Train Epoch: 20 [34000/50000 (68%)]\tLoss: 1735.499531\n",
      "Train Epoch: 20 [35000/50000 (70%)]\tLoss: 1786.536719\n",
      "Train Epoch: 20 [36000/50000 (72%)]\tLoss: 1755.325313\n",
      "Train Epoch: 20 [37000/50000 (74%)]\tLoss: 1758.583594\n",
      "Train Epoch: 20 [38000/50000 (76%)]\tLoss: 1758.444219\n",
      "Train Epoch: 20 [39000/50000 (78%)]\tLoss: 1764.439687\n",
      "Train Epoch: 20 [40000/50000 (80%)]\tLoss: 1744.137187\n",
      "Train Epoch: 20 [41000/50000 (82%)]\tLoss: 1764.175313\n",
      "Train Epoch: 20 [42000/50000 (84%)]\tLoss: 1745.813281\n",
      "Train Epoch: 20 [43000/50000 (86%)]\tLoss: 1757.874844\n",
      "Train Epoch: 20 [44000/50000 (88%)]\tLoss: 1824.944844\n",
      "Train Epoch: 20 [45000/50000 (90%)]\tLoss: 1708.453125\n",
      "Train Epoch: 20 [46000/50000 (92%)]\tLoss: 1736.730000\n",
      "Train Epoch: 20 [47000/50000 (94%)]\tLoss: 1783.517344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 20 [48000/50000 (96%)]\tLoss: 1767.867969\n",
      "Train Epoch: 20 [49000/50000 (98%)]\tLoss: 1782.692188\n",
      "====> Epoch: 20 Average loss: 1761.2986\n",
      "\n",
      "Test set: Average loss: 1762.5846\n",
      "\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 1827.418750\n",
      "Train Epoch: 21 [1000/50000 (2%)]\tLoss: 1750.690781\n",
      "Train Epoch: 21 [2000/50000 (4%)]\tLoss: 1748.391250\n",
      "Train Epoch: 21 [3000/50000 (6%)]\tLoss: 1754.673438\n",
      "Train Epoch: 21 [4000/50000 (8%)]\tLoss: 1769.774844\n",
      "Train Epoch: 21 [5000/50000 (10%)]\tLoss: 1784.069062\n",
      "Train Epoch: 21 [6000/50000 (12%)]\tLoss: 1742.513906\n",
      "Train Epoch: 21 [7000/50000 (14%)]\tLoss: 1747.748437\n",
      "Train Epoch: 21 [8000/50000 (16%)]\tLoss: 1747.962969\n",
      "Train Epoch: 21 [9000/50000 (18%)]\tLoss: 1771.650938\n",
      "Train Epoch: 21 [10000/50000 (20%)]\tLoss: 1719.444062\n",
      "Train Epoch: 21 [11000/50000 (22%)]\tLoss: 1777.383281\n",
      "Train Epoch: 21 [12000/50000 (24%)]\tLoss: 1735.165313\n",
      "Train Epoch: 21 [13000/50000 (26%)]\tLoss: 1759.469063\n",
      "Train Epoch: 21 [14000/50000 (28%)]\tLoss: 1794.662188\n",
      "Train Epoch: 21 [15000/50000 (30%)]\tLoss: 1775.801719\n",
      "Train Epoch: 21 [16000/50000 (32%)]\tLoss: 1766.732969\n",
      "Train Epoch: 21 [17000/50000 (34%)]\tLoss: 1803.872500\n",
      "Train Epoch: 21 [18000/50000 (36%)]\tLoss: 1772.742188\n",
      "Train Epoch: 21 [19000/50000 (38%)]\tLoss: 1732.175313\n",
      "Train Epoch: 21 [20000/50000 (40%)]\tLoss: 1781.169531\n",
      "Train Epoch: 21 [21000/50000 (42%)]\tLoss: 1748.642812\n",
      "Train Epoch: 21 [22000/50000 (44%)]\tLoss: 1766.399375\n",
      "Train Epoch: 21 [23000/50000 (46%)]\tLoss: 1811.485469\n",
      "Train Epoch: 21 [24000/50000 (48%)]\tLoss: 1726.069062\n",
      "Train Epoch: 21 [25000/50000 (50%)]\tLoss: 1783.386094\n",
      "Train Epoch: 21 [26000/50000 (52%)]\tLoss: 1750.452344\n",
      "Train Epoch: 21 [27000/50000 (54%)]\tLoss: 1778.369375\n",
      "Train Epoch: 21 [28000/50000 (56%)]\tLoss: 1737.760313\n",
      "Train Epoch: 21 [29000/50000 (58%)]\tLoss: 1746.785625\n",
      "Train Epoch: 21 [30000/50000 (60%)]\tLoss: 1761.497500\n",
      "Train Epoch: 21 [31000/50000 (62%)]\tLoss: 1763.748594\n",
      "Train Epoch: 21 [32000/50000 (64%)]\tLoss: 1753.456094\n",
      "Train Epoch: 21 [33000/50000 (66%)]\tLoss: 1777.630312\n",
      "Train Epoch: 21 [34000/50000 (68%)]\tLoss: 1739.954375\n",
      "Train Epoch: 21 [35000/50000 (70%)]\tLoss: 1793.860469\n",
      "Train Epoch: 21 [36000/50000 (72%)]\tLoss: 1762.875000\n",
      "Train Epoch: 21 [37000/50000 (74%)]\tLoss: 1771.497969\n",
      "Train Epoch: 21 [38000/50000 (76%)]\tLoss: 1764.693906\n",
      "Train Epoch: 21 [39000/50000 (78%)]\tLoss: 1743.669688\n",
      "Train Epoch: 21 [40000/50000 (80%)]\tLoss: 1774.753594\n",
      "Train Epoch: 21 [41000/50000 (82%)]\tLoss: 1742.297031\n",
      "Train Epoch: 21 [42000/50000 (84%)]\tLoss: 1801.044688\n",
      "Train Epoch: 21 [43000/50000 (86%)]\tLoss: 1774.124063\n",
      "Train Epoch: 21 [44000/50000 (88%)]\tLoss: 1744.438437\n",
      "Train Epoch: 21 [45000/50000 (90%)]\tLoss: 1718.826094\n",
      "Train Epoch: 21 [46000/50000 (92%)]\tLoss: 1724.375469\n",
      "Train Epoch: 21 [47000/50000 (94%)]\tLoss: 1739.218281\n",
      "Train Epoch: 21 [48000/50000 (96%)]\tLoss: 1780.677813\n",
      "Train Epoch: 21 [49000/50000 (98%)]\tLoss: 1741.285156\n",
      "====> Epoch: 21 Average loss: 1760.7364\n",
      "\n",
      "Test set: Average loss: 1765.9462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "main4(load_old=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
