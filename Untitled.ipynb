{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pdb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class arg():\n",
    "    seed=1\n",
    "    no_cuda=False\n",
    "\n",
    "    batch_size=64\n",
    "    intermediate_size=128 #usual hidden size, linear around z\n",
    "    hidden_size=30 # latent space z\n",
    "    test_batch_size=100\n",
    "    epochs=25\n",
    "    lr=1e-6 #0.001\n",
    "    momentum=0.5\n",
    "    log_interval=10\n",
    "    save_model=True\n",
    "    \n",
    "    cwd='D:/video_stash/thisenv/'\n",
    "    #NEW EXP\n",
    "    experiment=5\n",
    "    run_continued=True\n",
    "\n",
    "        \n",
    "    if not os.path.exists(os.path.join(cwd,f\"exp{experiment}\")):\n",
    "        os.makedirs(os.path.join(cwd,f\"exp{experiment}\"))\n",
    "        os.makedirs(os.path.join(cwd,f\"exp{experiment}/data\"))\n",
    "        open(os.path.join(cwd,f'exp{experiment}/logfile.txt'), 'w+').close()\n",
    "    elif not run_continued:\n",
    "        open(os.path.join(cwd,f'exp{experiment}/logfile.txt'), 'w+').close()\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "\n",
    "def do_write(string):\n",
    "    cwd='D:/video_stash/thisenv/'\n",
    "    f=open(os.path.join(cwd,f'exp5/logfile.txt'),'a+')\n",
    "    f.write(string)\n",
    "    f.close()\n",
    "\n",
    "def main4():\n",
    "    \n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            '''old: self.conv1 = nn.Conv2d(1, 20, 5, 1)#in out kernel_sz stride\n",
    "            self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "            self.fc1 = nn.Linear(4 * 4 * 50, 500) # in out\n",
    "            self.fc2 = nn.Linear(500, 10)'''\n",
    "             # Encoder\n",
    "            #32x32x3\n",
    "            self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)#32x32x32\n",
    "            self.conv2 = nn.Conv2d(32, 32, kernel_size=4, stride=4, padding=0)#8x8x32\n",
    "            self.conv3 = nn.Conv2d(32, 96, kernel_size=3, stride=1, padding=1)#8x8x96\n",
    "            self.conv3b = nn.Conv2d(96, 96, kernel_size=2, stride=3, padding=0)#3x3x96\n",
    "            self.conv4 = nn.Conv2d(96, 256, kernel_size=3, stride=1, padding=1)#3x3x256\n",
    "            self.fc1a = nn.Linear(3 * 3 * 256, 180)#2304\n",
    "            #FC 32x32/240x240= 0,177\n",
    "            #F*((I-K+2P)/S+1)\n",
    "            '''# Latent space\n",
    "            self.fc21 = nn.Linear(128, 20)\n",
    "            self.fc22 = nn.Linear(128, 20)'''\n",
    "            # Decoder\n",
    "            '''self.fc3 = nn.Linear(20, 128)'''\n",
    "\n",
    "            self.fc4a = nn.Linear(180, 2304)\n",
    "            self.deconv1 = nn.ConvTranspose2d(256, 96, kernel_size=3, stride=1, padding=1)\n",
    "            self.deconv1b = nn.ConvTranspose2d(96, 96, kernel_size=2, stride=3, padding=0)\n",
    "            self.deconv2 = nn.ConvTranspose2d(96, 32, kernel_size=3, stride=1, padding=1)\n",
    "            self.deconv3 = nn.ConvTranspose2d(32, 32, kernel_size=4, stride=4, padding=0)\n",
    "            self.conv5 = nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1)\n",
    "            \n",
    "            self.relu = nn.ReLU()\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "        def encode(self, x):\n",
    "\n",
    "            #do_write(\"Encoding: Convolution...\\n\")\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = F.relu(self.conv3(x))\n",
    "            x = F.relu(self.conv3b(x))\n",
    "            x = F.relu(self.conv4(x))\n",
    "       \n",
    "            x = x.view(x.size(0),-1)\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            #do_write(\"Encoding: FC...\\n\")\n",
    "            x = F.relu(self.fc1a(x))\n",
    "            \n",
    "            return x\n",
    "        \n",
    "        def normalize(self, x):\n",
    "            #x_normed = x / x.max(0, keepdim=True)[0] \n",
    "            #return x_normed\n",
    "            alpha=(x-x.mean(0,keepdim=True))\n",
    "            beta=alpha/x.std(0,keepdim=True)\n",
    "            return beta\n",
    "            \n",
    "        def decode(self, x):\n",
    "            #do_write(\"De-coding: FC...\\n\")\n",
    "            out = self.relu(self.fc4a(x))\n",
    "            # import pdb; pdb.set_trace()\n",
    "            out = out.view(out.size(0), 256, 3, 3)\n",
    "\n",
    "            #do_write(\"De-coding: Deconvolution...\\n\")\n",
    "            out = self.relu(self.deconv1(out))\n",
    "            out = self.relu(self.deconv1b(out))\n",
    "            out = self.relu(self.deconv2(out))\n",
    "            out = self.relu(self.deconv3(out))\n",
    "            out = self.sigmoid(self.conv5(out))\n",
    "            return out\n",
    "            \n",
    "        def forward(self, x):\n",
    "            mu = self.encode(x)\n",
    "            #return F.log_softmax(x, dim=1)\n",
    "            mu_0=self.normalize(mu)\n",
    "            #return self.decode(mu_0),mu_0\n",
    "            return self.decode(mu),mu\n",
    "        \n",
    "    '''def loss_function(recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x.view(-1, 32 * 32 * 3),\n",
    "                                 x.view(-1, 32 * 32 * 3), size_average=False)\n",
    "\n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # https://arxiv.org/abs/1312.6114\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        #kullbach-leibler divergence\n",
    "        return BCE + KLD'''\n",
    "    \n",
    "    def loss_function(recon_x, x):\n",
    "        #pdb.set_trace()\n",
    "        BCE = F.binary_cross_entropy(recon_x.view(-1, 32 * 32 * 3),\n",
    "                                 x.view(-1, 32 * 32 * 3), size_average=True)\n",
    "        return BCE \n",
    "    \n",
    "    def train(args, model, device, train_loader, optimizer, epoch):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)[0]\n",
    "\n",
    "            #do_write(\"Calc Loss...\\n\")\n",
    "            pdb.set_trace()\n",
    "            loss = loss_function(output, data)\n",
    "            loss.backward()\n",
    "            \n",
    "            #loss = F.nll_loss(output, target)\n",
    "            #loss.backward()\n",
    "            \n",
    "            #train_loss += loss.data[0]\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                do_write('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\n'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "        \n",
    "        do_write('====> Epoch: {} Average loss: {:.4f}\\n'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "        # save the reconstructed images\n",
    "        reconst_images = model(args.fixed_x)[0]\n",
    "        \n",
    "        ##\n",
    "        #print(my_variable.data.cpu().numpy())\n",
    "        #x = Variable()\n",
    "        #print(np.shape(reconst_images.data.cpu().numpy()[0]))\n",
    "        reconst_images = reconst_images.view(reconst_images.size(0), 3, 32, 32)\n",
    "        save_image(reconst_images.data.cpu(), os.path.join(args.cwd,f'./exp{args.experiment}/data/CIFAR_reconst_images_{epoch}.png'))\n",
    "\n",
    "    def test(args, model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        #correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)[0]\n",
    "                #nochma angucken\n",
    "                #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss, negative log likelihood loss.\n",
    "                \n",
    "                test_loss += loss_function(output, data).item()\n",
    "                #pdb.set_trace()\n",
    "                pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "                #correct += pred.eq(target.view_as(pred)).sum().item() #accuracy, elementwise equality, and sum\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "\n",
    "\n",
    "        do_write('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n",
    "    \n",
    "    args=arg()\n",
    "        \n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10(os.path.join(args.cwd,'data'), train=True, download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(os.path.join(args.cwd,'data'), train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    old_models=[]\n",
    "    if args.run_continued:\n",
    "        model=Net().to(device)\n",
    "        import glob\n",
    "        old_models=glob.glob(os.path.join(args.cwd,f\"exp{args.experiment}/cifar_cnn_*\"))\n",
    "        do_write(\"\\n load old model no \")\n",
    "        do_write(f\"{len(old_models)}\\n\")\n",
    "        model.load_state_dict(torch.load(os.path.join(args.cwd,f\"exp{args.experiment}/cifar_cnn_{len(old_models)}.pt\")))\n",
    "    else:\n",
    "        model = Net().to(device)\n",
    "    \n",
    "    #model = Net()\n",
    "    #if not args.no_cuda:\n",
    "    #    model.cuda()\n",
    "        \n",
    "        \n",
    "        \n",
    "    #optimizer = optim.RMSprop(model.parameters(), lr=args.lr)#2, lr=0.001 empfohlen\n",
    "    #optimizer = optim.RMSprop(model.parameters(), lr=args.lr)#3, lr=0.1\n",
    "    #optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)#4\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)#5\n",
    "    #gradients tend to vanish or explode\n",
    "    '''It uses a moving average of squared gradients to normalize the gradient itself. \n",
    "    That has an effect of balancing the step size?—?decrease the step for large gradient \n",
    "    to avoid exploding, and \n",
    "    increase the step for small gradient to avoid vanishing'''\n",
    "    \n",
    "    \n",
    "    #how to save fixed inputs for debugging\n",
    "    data_iter = iter(train_loader)\n",
    "    fixed_x, _ = next(data_iter)\n",
    "    #pdb.set_trace()\n",
    "    save_image(Variable(fixed_x).data.cpu(), os.path.join(args.cwd,f'./exp{args.experiment}/data/CIFAR_real_images.png'))\n",
    "    args.fixed_x = to_var(fixed_x) \n",
    "    #args.fixed_x = to_var(fixed_x.view(fixed_x.size(0), -1)) \n",
    "    #args.fixed_x=args.fixed_x.to(device)\n",
    "    #print(np.shape(args.fixed_x.data.cpu().numpy()))\n",
    "    \n",
    "    for epoch in range(len(old_models)+1, args.epochs + len(old_models)+1):\n",
    "        do_write(\"in epoch\")\n",
    "        do_write(f\"{epoch}\\n\")\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader)\n",
    "\n",
    "        if (args.save_model):\n",
    "            torch.save(model.state_dict(), os.path.join(args.cwd,f\"exp{args.experiment}/cifar_cnn_{epoch}.pt\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "main4()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
