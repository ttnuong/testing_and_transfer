{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pdb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trixi.logger.experiment.pytorchexperimentlogger import PytorchExperimentLogger\n",
    "from trixi.logger import PytorchVisdomLogger\n",
    "from trixi.util import Config\n",
    "\n",
    "Exp = PytorchExperimentLogger(base_dir=\"./experiment_dir\", \n",
    "                              experiment_name=\"test-experiment\",\n",
    "                              folder_format=\"{experiment_name}\")\n",
    "\n",
    "Viz = PytorchVisdomLogger(name=\"main\", port=8080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arg():\n",
    "    seed=1\n",
    "    no_cuda=True\n",
    "    batch_size=100\n",
    "    intermediate_size=128 #usual hidden size, linear around z\n",
    "    hidden_size=30 # latent space z\n",
    "    test_batch_size=100\n",
    "    epochs=5\n",
    "    lr=1e-1 #0.001\n",
    "    momentum=0.5\n",
    "    log_interval=10\n",
    "    save_model=True\n",
    "        \n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 16 * 32, 128)#FC 128\n",
    "        #F*((Iâˆ’K+2P)/S+1)\n",
    "        \n",
    "        # Latent space\n",
    "        self.fc21 = nn.Linear(128, 20)\n",
    "        self.fc22 = nn.Linear(128, 20)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(20, 128)\n",
    "        self.fc4 = nn.Linear(128, 8192)\n",
    "        self.deconv1 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 32, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv5 = nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def encode(self, x):\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.relu(self.conv2(out))\n",
    "        out = self.relu(self.conv3(out))\n",
    "        out = self.relu(self.conv4(out))\n",
    "        out = out.view(out.size(0), -1)\n",
    "        h1 = self.relu(self.fc1(out))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = self.relu(self.fc3(z))\n",
    "        out = self.relu(self.fc4(h3))\n",
    "        # import pdb; pdb.set_trace()\n",
    "        out = out.view(out.size(0), 32, 16, 16)\n",
    "        out = self.relu(self.deconv1(out))\n",
    "        out = self.relu(self.deconv2(out))\n",
    "        out = self.relu(self.deconv3(out))\n",
    "        out = self.sigmoid(self.conv5(out))\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x) \n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def main3():\n",
    "    args=arg()\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if not args.no_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=True, download=True,\n",
    "                     transform=transforms.ToTensor()),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('../data', train=False, transform=transforms.ToTensor()),\n",
    "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "    \n",
    "    #https://stackoverflow.com/questions/37512290/reading-cifar10-dataset-in-batches\n",
    "    data_iter = iter(train_loader)\n",
    "    \n",
    "    #how to save fixed inputs for debugging\n",
    "    fixed_x, _ = next(data_iter)\n",
    "    save_image(Variable(fixed_x).data.cpu(), './data/vae_real_images.png')\n",
    "    #args.fixed_x = to_var(fixed_x.view(fixed_x.size(0), -1)) #nur erstes batch i think als baeline\n",
    "    args.fixed_x = Variable(fixed_x, volatile=True)\n",
    "    #args.fixed_x = to_var(fixed_x)\n",
    "    model = VAE()\n",
    "    if not args.no_cuda:\n",
    "        model.cuda()\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "        \n",
    "    def loss_function(recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x.view(-1, 32 * 32 * 3),\n",
    "                                 x.view(-1, 32 * 32 * 3), size_average=False)\n",
    "\n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # https://arxiv.org/abs/1312.6114\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        #kullbach-leibler divergence\n",
    "        return BCE + KLD\n",
    "\n",
    "\n",
    "    def train(epoch):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, _) in enumerate(train_loader):\n",
    "            data = Variable(data)\n",
    "            if not args.no_cuda:\n",
    "                data = data.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.item() / len(data)))\n",
    "\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "              epoch, train_loss / len(train_loader.dataset)))\n",
    "        \n",
    "        # save the reconstructed images\n",
    "        reconst_images, mu, logvar  = model(args.fixed_x)\n",
    "        reconst_images = reconst_images.view(reconst_images.size(0), 3, 32, 32)\n",
    "        save_image(reconst_images.data.cpu(), './data/vae_reconst_images_%d.png' % (epoch))\n",
    "\n",
    "\n",
    "    def test(epoch):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            if not args.no_cuda:\n",
    "                data = data.cuda()\n",
    "            data = Variable(data, volatile=True)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            '''            if epoch == args.epochs and i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                       recon_batch[:n]])\n",
    "                save_image(comparison.data.cpu(),\n",
    "                           'snapshots/conv_vae/reconstruction_' + str(epoch) +\n",
    "                           '.png', nrow=n)'''\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(epoch)\n",
    "        test(epoch)\n",
    "        '''if epoch == args.epochs:\n",
    "            sample = Variable(torch.randn(64, args.hidden_size))\n",
    "            if not args.no_cuda:\n",
    "                sample = sample.cuda()\n",
    "            sample = model.decode(sample).cpu()\n",
    "            save_image(sample.data.view(64, 3, 32, 32),\n",
    "                       'snapshots/conv_vae/sample_' + str(epoch) + '.png')'''\n",
    "        torch.save(model.state_dict(), \"vae_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main4(load_old=False):\n",
    "    \n",
    "    class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            '''old: self.conv1 = nn.Conv2d(1, 20, 5, 1)#in out kernel_sz stride\n",
    "            self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "            self.fc1 = nn.Linear(4 * 4 * 50, 500) # in out\n",
    "            self.fc2 = nn.Linear(500, 10)'''\n",
    "             # Encoder\n",
    "            self.conv1 = nn.Conv2d(3, 3, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv2 = nn.Conv2d(3, 32, kernel_size=2, stride=2, padding=0)\n",
    "            self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv4 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "            self.fc1 = nn.Linear(16 * 16 * 32, 128)#FC 128\n",
    "            #F*((Iâˆ’K+2P)/S+1)\n",
    "            \n",
    "            '''# Latent space\n",
    "            self.fc21 = nn.Linear(128, 20)\n",
    "            self.fc22 = nn.Linear(128, 20)'''\n",
    "            # Decoder\n",
    "            '''self.fc3 = nn.Linear(20, 128)'''\n",
    "            self.fc4 = nn.Linear(128, 8192)\n",
    "            self.deconv1 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "            self.deconv2 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "            self.deconv3 = nn.ConvTranspose2d(32, 32, kernel_size=2, stride=2, padding=0)\n",
    "            self.conv5 = nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1)\n",
    "            \n",
    "            self.relu = nn.ReLU()\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "            \n",
    "        def encode(self, x):\n",
    "            x = F.relu(self.conv1(x))\n",
    "            #x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            #x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.relu(self.conv3(x))\n",
    "            #x = F.max_pool2d(x, 2, 2)\n",
    "            x = F.relu(self.conv4(x))\n",
    "            #x = F.max_pool2d(x, 2, 2)\n",
    "            \n",
    "            x = x.view(x.size(0),-1)\n",
    "            \n",
    "            return F.relu(self.fc1(x))\n",
    "        def normalize(self, x):\n",
    "            #x_normed = x / x.max(0, keepdim=True)[0] \n",
    "            #return x_normed\n",
    "            alpha=(x-x.mean(0,keepdim=True))\n",
    "            beta=alpha/x.std(0,keepdim=True)\n",
    "            return beta\n",
    "            \n",
    "        def decode(self, x):\n",
    "            out = self.relu(self.fc4(x))\n",
    "            # import pdb; pdb.set_trace()\n",
    "            out = out.view(out.size(0), 32, 16, 16)\n",
    "            out = self.relu(self.deconv1(out))\n",
    "            out = self.relu(self.deconv2(out))\n",
    "            out = self.relu(self.deconv3(out))\n",
    "            out = self.sigmoid(self.conv5(out))\n",
    "            return out\n",
    "            \n",
    "        def forward(self, x):\n",
    "            mu = self.encode(x)\n",
    "            #return F.log_softmax(x, dim=1)\n",
    "            mu_0=self.normalize(mu)\n",
    "            #return self.decode(mu_0),mu_0\n",
    "            return self.decode(mu),mu\n",
    "        \n",
    "    '''def loss_function(recon_x, x, mu, logvar):\n",
    "        BCE = F.binary_cross_entropy(recon_x.view(-1, 32 * 32 * 3),\n",
    "                                 x.view(-1, 32 * 32 * 3), size_average=False)\n",
    "\n",
    "        # see Appendix B from VAE paper:\n",
    "        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "        # https://arxiv.org/abs/1312.6114\n",
    "        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        #kullbach-leibler divergence\n",
    "        return BCE + KLD'''\n",
    "    \n",
    "    def loss_function(recon_x, x):\n",
    "        #pdb.set_trace()\n",
    "        BCE = F.binary_cross_entropy(recon_x.view(-1, 32 * 32 * 3),\n",
    "                                 x.view(-1, 32 * 32 * 3), size_average=False)\n",
    "        return BCE \n",
    "    \n",
    "    def train(args, model, device, train_loader, optimizer, epoch):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)[0]\n",
    "            loss = loss_function(output, data)\n",
    "            loss.backward()\n",
    "            \n",
    "            #loss = F.nll_loss(output, target)\n",
    "            #loss.backward()\n",
    "            \n",
    "            #train_loss += loss.data[0]\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            optimizer.step()\n",
    "            '''if batch_idx % args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()))'''\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.item() / len(data)))\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "        # save the reconstructed images\n",
    "        reconst_images = model(args.fixed_x)[0]\n",
    "        \n",
    "        ##\n",
    "        #print(my_variable.data.cpu().numpy())\n",
    "        #x = Variable()\n",
    "        #print(np.shape(reconst_images.data.cpu().numpy()[0]))\n",
    "        reconst_images = reconst_images.view(reconst_images.size(0), 3, 32, 32)\n",
    "        save_image(reconst_images.data.cpu(), './data/CIFAR_reconst_images_%d.png' % (epoch))\n",
    "\n",
    "    def test(args, model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        #correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)[0]\n",
    "                #nochma angucken\n",
    "                #test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss, negative log likelihood loss.\n",
    "                \n",
    "                test_loss += loss_function(output, data).item()\n",
    "                #pdb.set_trace()\n",
    "                pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "                #correct += pred.eq(target.view_as(pred)).sum().item() #accuracy, elementwise equality, and sum\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        print('\\nTest set: Average loss: {:.4f}\\n'.format(\n",
    "            test_loss))\n",
    "    \n",
    "    \n",
    "    args=arg()  \n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('../data', train=True, download=True,\n",
    "                      transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "    batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "    \n",
    "    \n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    if load_old:\n",
    "        model=Net().to(device)\n",
    "        model.load_state_dict(torch.load(\"cifar_cnn_3.pt\"))\n",
    "    else:\n",
    "        model = Net().to(device)\n",
    "    \n",
    "    #model = Net()\n",
    "    #if not args.no_cuda:\n",
    "    #    model.cuda()\n",
    "        \n",
    "    #optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
    "    #gradients tend to vanish or explode\n",
    "    '''It uses a moving average of squared gradients to normalize the gradient itself. \n",
    "    That has an effect of balancing the step sizeâ€Šâ€”â€Šdecrease the step for large gradient \n",
    "    to avoid exploding, and \n",
    "    increase the step for small gradient to avoid vanishing'''\n",
    "    \n",
    "    \n",
    "    #how to save fixed inputs for debugging\n",
    "    data_iter = iter(train_loader)\n",
    "    fixed_x, _ = next(data_iter)\n",
    "    #pdb.set_trace()\n",
    "    save_image(Variable(fixed_x).data.cpu(), './data/CIFAR_real_images.png')\n",
    "    args.fixed_x = to_var(fixed_x) \n",
    "    #args.fixed_x = to_var(fixed_x.view(fixed_x.size(0), -1)) \n",
    "    #args.fixed_x=args.fixed_x.to(device)\n",
    "    #print(np.shape(args.fixed_x.data.cpu().numpy()))\n",
    "    \n",
    "    for epoch in range(9, args.epochs + 9):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader)\n",
    "\n",
    "        if (args.save_model):\n",
    "            torch.save(model.state_dict(), f\"cifar_cnn_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2127.722500\n",
      "Train Epoch: 1 [1000/50000 (2%)]\tLoss: 2116.976250\n",
      "Train Epoch: 1 [2000/50000 (4%)]\tLoss: 2118.335000\n",
      "Train Epoch: 1 [3000/50000 (6%)]\tLoss: 2098.944219\n",
      "Train Epoch: 1 [4000/50000 (8%)]\tLoss: 2182.098125\n",
      "Train Epoch: 1 [5000/50000 (10%)]\tLoss: 2099.554531\n",
      "Train Epoch: 1 [6000/50000 (12%)]\tLoss: 2001.332031\n",
      "Train Epoch: 1 [7000/50000 (14%)]\tLoss: 1991.417813\n",
      "Train Epoch: 1 [8000/50000 (16%)]\tLoss: 2000.685469\n",
      "Train Epoch: 1 [9000/50000 (18%)]\tLoss: 2011.515938\n",
      "Train Epoch: 1 [10000/50000 (20%)]\tLoss: 2013.734688\n",
      "Train Epoch: 1 [11000/50000 (22%)]\tLoss: 2005.849062\n",
      "Train Epoch: 1 [12000/50000 (24%)]\tLoss: 1937.953750\n",
      "Train Epoch: 1 [13000/50000 (26%)]\tLoss: 2013.366875\n",
      "Train Epoch: 1 [14000/50000 (28%)]\tLoss: 1899.362656\n",
      "Train Epoch: 1 [15000/50000 (30%)]\tLoss: 1923.627656\n",
      "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 1902.885313\n",
      "Train Epoch: 1 [17000/50000 (34%)]\tLoss: 1920.427187\n",
      "Train Epoch: 1 [18000/50000 (36%)]\tLoss: 1886.350312\n",
      "Train Epoch: 1 [19000/50000 (38%)]\tLoss: 1956.141875\n",
      "Train Epoch: 1 [20000/50000 (40%)]\tLoss: 1889.157344\n",
      "Train Epoch: 1 [21000/50000 (42%)]\tLoss: 1903.288281\n",
      "Train Epoch: 1 [22000/50000 (44%)]\tLoss: 1947.425156\n",
      "Train Epoch: 1 [23000/50000 (46%)]\tLoss: 1908.129531\n",
      "Train Epoch: 1 [24000/50000 (48%)]\tLoss: 1934.980937\n",
      "Train Epoch: 1 [25000/50000 (50%)]\tLoss: 1883.023125\n",
      "Train Epoch: 1 [26000/50000 (52%)]\tLoss: 1864.879844\n",
      "Train Epoch: 1 [27000/50000 (54%)]\tLoss: 1913.997813\n",
      "Train Epoch: 1 [28000/50000 (56%)]\tLoss: 1889.348281\n",
      "Train Epoch: 1 [29000/50000 (58%)]\tLoss: 1910.973438\n",
      "Train Epoch: 1 [30000/50000 (60%)]\tLoss: 1873.123594\n",
      "Train Epoch: 1 [31000/50000 (62%)]\tLoss: 1876.423125\n",
      "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1896.936406\n",
      "Train Epoch: 1 [33000/50000 (66%)]\tLoss: 1904.654844\n",
      "Train Epoch: 1 [34000/50000 (68%)]\tLoss: 1847.171250\n",
      "Train Epoch: 1 [35000/50000 (70%)]\tLoss: 1877.156094\n",
      "Train Epoch: 1 [36000/50000 (72%)]\tLoss: 1870.517969\n",
      "Train Epoch: 1 [37000/50000 (74%)]\tLoss: 1856.352969\n",
      "Train Epoch: 1 [38000/50000 (76%)]\tLoss: 1860.614531\n",
      "Train Epoch: 1 [39000/50000 (78%)]\tLoss: 1813.830625\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 1898.252812\n",
      "Train Epoch: 1 [41000/50000 (82%)]\tLoss: 1876.550469\n",
      "Train Epoch: 1 [42000/50000 (84%)]\tLoss: 1860.488437\n",
      "Train Epoch: 1 [43000/50000 (86%)]\tLoss: 1845.268125\n",
      "Train Epoch: 1 [44000/50000 (88%)]\tLoss: 1894.339844\n",
      "Train Epoch: 1 [45000/50000 (90%)]\tLoss: 1888.706875\n",
      "Train Epoch: 1 [46000/50000 (92%)]\tLoss: 1893.537188\n",
      "Train Epoch: 1 [47000/50000 (94%)]\tLoss: 1902.481094\n",
      "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1901.236562\n",
      "Train Epoch: 1 [49000/50000 (98%)]\tLoss: 1890.706719\n",
      "====> Epoch: 1 Average loss: 1932.5782\n",
      "\n",
      "Test set: Average loss: 1870.0265\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1888.756094\n",
      "Train Epoch: 2 [1000/50000 (2%)]\tLoss: 1838.607656\n",
      "Train Epoch: 2 [2000/50000 (4%)]\tLoss: 1878.343281\n",
      "Train Epoch: 2 [3000/50000 (6%)]\tLoss: 1824.676250\n",
      "Train Epoch: 2 [4000/50000 (8%)]\tLoss: 1825.482188\n",
      "Train Epoch: 2 [5000/50000 (10%)]\tLoss: 1880.422031\n",
      "Train Epoch: 2 [6000/50000 (12%)]\tLoss: 1897.203438\n",
      "Train Epoch: 2 [7000/50000 (14%)]\tLoss: 1830.407813\n",
      "Train Epoch: 2 [8000/50000 (16%)]\tLoss: 1849.168437\n",
      "Train Epoch: 2 [9000/50000 (18%)]\tLoss: 1848.778281\n",
      "Train Epoch: 2 [10000/50000 (20%)]\tLoss: 1841.003750\n",
      "Train Epoch: 2 [11000/50000 (22%)]\tLoss: 1916.561719\n",
      "Train Epoch: 2 [12000/50000 (24%)]\tLoss: 1855.059375\n",
      "Train Epoch: 2 [13000/50000 (26%)]\tLoss: 1857.536250\n",
      "Train Epoch: 2 [14000/50000 (28%)]\tLoss: 1815.886875\n",
      "Train Epoch: 2 [15000/50000 (30%)]\tLoss: 1857.389219\n",
      "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 1834.441719\n",
      "Train Epoch: 2 [17000/50000 (34%)]\tLoss: 1853.718125\n",
      "Train Epoch: 2 [18000/50000 (36%)]\tLoss: 1852.247813\n",
      "Train Epoch: 2 [19000/50000 (38%)]\tLoss: 1828.996875\n",
      "Train Epoch: 2 [20000/50000 (40%)]\tLoss: 1850.170000\n",
      "Train Epoch: 2 [21000/50000 (42%)]\tLoss: 1795.483281\n",
      "Train Epoch: 2 [22000/50000 (44%)]\tLoss: 1847.460625\n",
      "Train Epoch: 2 [23000/50000 (46%)]\tLoss: 1895.276875\n",
      "Train Epoch: 2 [24000/50000 (48%)]\tLoss: 1842.104687\n",
      "Train Epoch: 2 [25000/50000 (50%)]\tLoss: 1838.114063\n",
      "Train Epoch: 2 [26000/50000 (52%)]\tLoss: 1880.881719\n",
      "Train Epoch: 2 [27000/50000 (54%)]\tLoss: 1795.325625\n",
      "Train Epoch: 2 [28000/50000 (56%)]\tLoss: 1843.888750\n",
      "Train Epoch: 2 [29000/50000 (58%)]\tLoss: 1872.794063\n",
      "Train Epoch: 2 [30000/50000 (60%)]\tLoss: 1850.165000\n",
      "Train Epoch: 2 [31000/50000 (62%)]\tLoss: 1849.052031\n",
      "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1866.456406\n",
      "Train Epoch: 2 [33000/50000 (66%)]\tLoss: 1778.826406\n",
      "Train Epoch: 2 [34000/50000 (68%)]\tLoss: 1829.325313\n",
      "Train Epoch: 2 [35000/50000 (70%)]\tLoss: 1879.897813\n",
      "Train Epoch: 2 [36000/50000 (72%)]\tLoss: 1813.633906\n",
      "Train Epoch: 2 [37000/50000 (74%)]\tLoss: 1863.452031\n",
      "Train Epoch: 2 [38000/50000 (76%)]\tLoss: 1869.298125\n",
      "Train Epoch: 2 [39000/50000 (78%)]\tLoss: 1799.965000\n",
      "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 1848.109219\n",
      "Train Epoch: 2 [41000/50000 (82%)]\tLoss: 1862.564531\n",
      "Train Epoch: 2 [42000/50000 (84%)]\tLoss: 1804.941562\n",
      "Train Epoch: 2 [43000/50000 (86%)]\tLoss: 1835.532969\n",
      "Train Epoch: 2 [44000/50000 (88%)]\tLoss: 1842.273281\n",
      "Train Epoch: 2 [45000/50000 (90%)]\tLoss: 1834.662031\n",
      "Train Epoch: 2 [46000/50000 (92%)]\tLoss: 1806.751875\n",
      "Train Epoch: 2 [47000/50000 (94%)]\tLoss: 1854.435937\n",
      "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1812.336875\n",
      "Train Epoch: 2 [49000/50000 (98%)]\tLoss: 1793.590156\n",
      "====> Epoch: 2 Average loss: 1844.8794\n",
      "\n",
      "Test set: Average loss: 1830.1261\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1811.978437\n",
      "Train Epoch: 3 [1000/50000 (2%)]\tLoss: 1819.668437\n",
      "Train Epoch: 3 [2000/50000 (4%)]\tLoss: 1813.072187\n",
      "Train Epoch: 3 [3000/50000 (6%)]\tLoss: 1819.769375\n",
      "Train Epoch: 3 [4000/50000 (8%)]\tLoss: 1838.904219\n",
      "Train Epoch: 3 [5000/50000 (10%)]\tLoss: 1810.347500\n",
      "Train Epoch: 3 [6000/50000 (12%)]\tLoss: 1868.409688\n",
      "Train Epoch: 3 [7000/50000 (14%)]\tLoss: 1825.531250\n",
      "Train Epoch: 3 [8000/50000 (16%)]\tLoss: 1822.101406\n",
      "Train Epoch: 3 [9000/50000 (18%)]\tLoss: 1841.745781\n",
      "Train Epoch: 3 [10000/50000 (20%)]\tLoss: 1790.819844\n",
      "Train Epoch: 3 [11000/50000 (22%)]\tLoss: 1821.790000\n",
      "Train Epoch: 3 [12000/50000 (24%)]\tLoss: 1835.249531\n",
      "Train Epoch: 3 [13000/50000 (26%)]\tLoss: 1834.270469\n",
      "Train Epoch: 3 [14000/50000 (28%)]\tLoss: 1855.677969\n",
      "Train Epoch: 3 [15000/50000 (30%)]\tLoss: 1820.225000\n",
      "Train Epoch: 3 [16000/50000 (32%)]\tLoss: 1822.135156\n",
      "Train Epoch: 3 [17000/50000 (34%)]\tLoss: 1860.470781\n",
      "Train Epoch: 3 [18000/50000 (36%)]\tLoss: 1856.451719\n",
      "Train Epoch: 3 [19000/50000 (38%)]\tLoss: 1833.115313\n",
      "Train Epoch: 3 [20000/50000 (40%)]\tLoss: 1816.108906\n",
      "Train Epoch: 3 [21000/50000 (42%)]\tLoss: 1795.356406\n",
      "Train Epoch: 3 [22000/50000 (44%)]\tLoss: 1859.991563\n",
      "Train Epoch: 3 [23000/50000 (46%)]\tLoss: 1831.478281\n",
      "Train Epoch: 3 [24000/50000 (48%)]\tLoss: 1782.917344\n",
      "Train Epoch: 3 [25000/50000 (50%)]\tLoss: 1833.895313\n",
      "Train Epoch: 3 [26000/50000 (52%)]\tLoss: 1842.425156\n",
      "Train Epoch: 3 [27000/50000 (54%)]\tLoss: 1818.414375\n",
      "Train Epoch: 3 [28000/50000 (56%)]\tLoss: 1818.455156\n",
      "Train Epoch: 3 [29000/50000 (58%)]\tLoss: 1811.394063\n",
      "Train Epoch: 3 [30000/50000 (60%)]\tLoss: 1839.075000\n",
      "Train Epoch: 3 [31000/50000 (62%)]\tLoss: 1859.969531\n",
      "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1763.876094\n",
      "Train Epoch: 3 [33000/50000 (66%)]\tLoss: 1801.501719\n",
      "Train Epoch: 3 [34000/50000 (68%)]\tLoss: 1817.532500\n",
      "Train Epoch: 3 [35000/50000 (70%)]\tLoss: 1818.671250\n",
      "Train Epoch: 3 [36000/50000 (72%)]\tLoss: 1819.915937\n",
      "Train Epoch: 3 [37000/50000 (74%)]\tLoss: 1845.695937\n",
      "Train Epoch: 3 [38000/50000 (76%)]\tLoss: 1799.505156\n",
      "Train Epoch: 3 [39000/50000 (78%)]\tLoss: 1822.922969\n",
      "Train Epoch: 3 [40000/50000 (80%)]\tLoss: 1822.302187\n",
      "Train Epoch: 3 [41000/50000 (82%)]\tLoss: 1822.677031\n",
      "Train Epoch: 3 [42000/50000 (84%)]\tLoss: 1766.711719\n",
      "Train Epoch: 3 [43000/50000 (86%)]\tLoss: 1815.468437\n",
      "Train Epoch: 3 [44000/50000 (88%)]\tLoss: 1817.518750\n",
      "Train Epoch: 3 [45000/50000 (90%)]\tLoss: 1818.373281\n",
      "Train Epoch: 3 [46000/50000 (92%)]\tLoss: 1817.225938\n",
      "Train Epoch: 3 [47000/50000 (94%)]\tLoss: 1801.980313\n",
      "Train Epoch: 3 [48000/50000 (96%)]\tLoss: 1790.107812\n",
      "Train Epoch: 3 [49000/50000 (98%)]\tLoss: 1832.275625\n",
      "====> Epoch: 3 Average loss: 1823.1202\n",
      "\n",
      "Test set: Average loss: 1803.8069\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1803.121250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [1000/50000 (2%)]\tLoss: 1778.044688\n",
      "Train Epoch: 4 [2000/50000 (4%)]\tLoss: 1805.629219\n",
      "Train Epoch: 4 [3000/50000 (6%)]\tLoss: 1811.555469\n",
      "Train Epoch: 4 [4000/50000 (8%)]\tLoss: 1823.006719\n",
      "Train Epoch: 4 [5000/50000 (10%)]\tLoss: 1798.974375\n",
      "Train Epoch: 4 [6000/50000 (12%)]\tLoss: 1785.298750\n",
      "Train Epoch: 4 [7000/50000 (14%)]\tLoss: 1843.603437\n",
      "Train Epoch: 4 [8000/50000 (16%)]\tLoss: 1822.619062\n",
      "Train Epoch: 4 [9000/50000 (18%)]\tLoss: 1804.399688\n",
      "Train Epoch: 4 [10000/50000 (20%)]\tLoss: 1830.427031\n",
      "Train Epoch: 4 [11000/50000 (22%)]\tLoss: 1825.886406\n",
      "Train Epoch: 4 [12000/50000 (24%)]\tLoss: 1829.982188\n",
      "Train Epoch: 4 [13000/50000 (26%)]\tLoss: 1765.287812\n",
      "Train Epoch: 4 [14000/50000 (28%)]\tLoss: 1810.230937\n",
      "Train Epoch: 4 [15000/50000 (30%)]\tLoss: 1805.215469\n",
      "Train Epoch: 4 [16000/50000 (32%)]\tLoss: 1797.407187\n",
      "Train Epoch: 4 [17000/50000 (34%)]\tLoss: 1812.321094\n",
      "Train Epoch: 4 [18000/50000 (36%)]\tLoss: 1782.374687\n",
      "Train Epoch: 4 [19000/50000 (38%)]\tLoss: 1800.274219\n",
      "Train Epoch: 4 [20000/50000 (40%)]\tLoss: 1790.119688\n",
      "Train Epoch: 4 [21000/50000 (42%)]\tLoss: 1790.566094\n",
      "Train Epoch: 4 [22000/50000 (44%)]\tLoss: 1780.771719\n",
      "Train Epoch: 4 [23000/50000 (46%)]\tLoss: 1788.550313\n",
      "Train Epoch: 4 [24000/50000 (48%)]\tLoss: 1825.842656\n",
      "Train Epoch: 4 [25000/50000 (50%)]\tLoss: 1776.553594\n",
      "Train Epoch: 4 [26000/50000 (52%)]\tLoss: 1739.550000\n",
      "Train Epoch: 4 [27000/50000 (54%)]\tLoss: 1795.644063\n",
      "Train Epoch: 4 [28000/50000 (56%)]\tLoss: 1800.171875\n",
      "Train Epoch: 4 [29000/50000 (58%)]\tLoss: 1816.235469\n",
      "Train Epoch: 4 [30000/50000 (60%)]\tLoss: 1826.555938\n",
      "Train Epoch: 4 [31000/50000 (62%)]\tLoss: 1820.197187\n",
      "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1867.417187\n",
      "Train Epoch: 4 [33000/50000 (66%)]\tLoss: 1793.570625\n",
      "Train Epoch: 4 [34000/50000 (68%)]\tLoss: 1772.662344\n",
      "Train Epoch: 4 [35000/50000 (70%)]\tLoss: 1817.133594\n",
      "Train Epoch: 4 [36000/50000 (72%)]\tLoss: 1787.049531\n",
      "Train Epoch: 4 [37000/50000 (74%)]\tLoss: 1806.094219\n",
      "Train Epoch: 4 [38000/50000 (76%)]\tLoss: 1824.252031\n",
      "Train Epoch: 4 [39000/50000 (78%)]\tLoss: 1833.302031\n",
      "Train Epoch: 4 [40000/50000 (80%)]\tLoss: 1763.850625\n",
      "Train Epoch: 4 [41000/50000 (82%)]\tLoss: 1770.426719\n",
      "Train Epoch: 4 [42000/50000 (84%)]\tLoss: 1799.302031\n",
      "Train Epoch: 4 [43000/50000 (86%)]\tLoss: 1774.797969\n",
      "Train Epoch: 4 [44000/50000 (88%)]\tLoss: 1822.073437\n",
      "Train Epoch: 4 [45000/50000 (90%)]\tLoss: 1788.879375\n",
      "Train Epoch: 4 [46000/50000 (92%)]\tLoss: 1775.767031\n",
      "Train Epoch: 4 [47000/50000 (94%)]\tLoss: 1807.696406\n",
      "Train Epoch: 4 [48000/50000 (96%)]\tLoss: 1728.838125\n",
      "Train Epoch: 4 [49000/50000 (98%)]\tLoss: 1787.922969\n",
      "====> Epoch: 4 Average loss: 1802.5891\n",
      "\n",
      "Test set: Average loss: 1791.8567\n",
      "\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1739.966094\n",
      "Train Epoch: 5 [1000/50000 (2%)]\tLoss: 1758.630000\n",
      "Train Epoch: 5 [2000/50000 (4%)]\tLoss: 1792.792656\n",
      "Train Epoch: 5 [3000/50000 (6%)]\tLoss: 1738.063125\n",
      "Train Epoch: 5 [4000/50000 (8%)]\tLoss: 1768.409219\n",
      "Train Epoch: 5 [5000/50000 (10%)]\tLoss: 1804.999375\n",
      "Train Epoch: 5 [6000/50000 (12%)]\tLoss: 1779.813906\n",
      "Train Epoch: 5 [7000/50000 (14%)]\tLoss: 1862.894687\n",
      "Train Epoch: 5 [8000/50000 (16%)]\tLoss: 1791.980781\n",
      "Train Epoch: 5 [9000/50000 (18%)]\tLoss: 1776.729375\n",
      "Train Epoch: 5 [10000/50000 (20%)]\tLoss: 1796.824531\n",
      "Train Epoch: 5 [11000/50000 (22%)]\tLoss: 1777.596094\n",
      "Train Epoch: 5 [12000/50000 (24%)]\tLoss: 1818.693906\n",
      "Train Epoch: 5 [13000/50000 (26%)]\tLoss: 1813.592813\n",
      "Train Epoch: 5 [14000/50000 (28%)]\tLoss: 1771.390625\n",
      "Train Epoch: 5 [15000/50000 (30%)]\tLoss: 1790.847969\n",
      "Train Epoch: 5 [16000/50000 (32%)]\tLoss: 1792.306094\n",
      "Train Epoch: 5 [17000/50000 (34%)]\tLoss: 1806.763125\n",
      "Train Epoch: 5 [18000/50000 (36%)]\tLoss: 1801.307500\n",
      "Train Epoch: 5 [19000/50000 (38%)]\tLoss: 1752.166875\n",
      "Train Epoch: 5 [20000/50000 (40%)]\tLoss: 1790.958125\n",
      "Train Epoch: 5 [21000/50000 (42%)]\tLoss: 1798.401250\n",
      "Train Epoch: 5 [22000/50000 (44%)]\tLoss: 1769.315781\n",
      "Train Epoch: 5 [23000/50000 (46%)]\tLoss: 1785.880000\n",
      "Train Epoch: 5 [24000/50000 (48%)]\tLoss: 1808.008750\n",
      "Train Epoch: 5 [25000/50000 (50%)]\tLoss: 1795.465938\n",
      "Train Epoch: 5 [26000/50000 (52%)]\tLoss: 1775.829844\n",
      "Train Epoch: 5 [27000/50000 (54%)]\tLoss: 1809.270937\n",
      "Train Epoch: 5 [28000/50000 (56%)]\tLoss: 1777.877031\n",
      "Train Epoch: 5 [29000/50000 (58%)]\tLoss: 1773.299531\n",
      "Train Epoch: 5 [30000/50000 (60%)]\tLoss: 1779.851719\n",
      "Train Epoch: 5 [31000/50000 (62%)]\tLoss: 1828.842344\n",
      "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1795.051563\n",
      "Train Epoch: 5 [33000/50000 (66%)]\tLoss: 1757.133594\n",
      "Train Epoch: 5 [34000/50000 (68%)]\tLoss: 1764.641562\n",
      "Train Epoch: 5 [35000/50000 (70%)]\tLoss: 1800.150469\n",
      "Train Epoch: 5 [36000/50000 (72%)]\tLoss: 1791.846250\n",
      "Train Epoch: 5 [37000/50000 (74%)]\tLoss: 1768.263438\n",
      "Train Epoch: 5 [38000/50000 (76%)]\tLoss: 1818.685937\n",
      "Train Epoch: 5 [39000/50000 (78%)]\tLoss: 1801.992500\n",
      "Train Epoch: 5 [40000/50000 (80%)]\tLoss: 1746.417656\n",
      "Train Epoch: 5 [41000/50000 (82%)]\tLoss: 1755.023594\n",
      "Train Epoch: 5 [42000/50000 (84%)]\tLoss: 1791.234531\n",
      "Train Epoch: 5 [43000/50000 (86%)]\tLoss: 1767.384844\n",
      "Train Epoch: 5 [44000/50000 (88%)]\tLoss: 1791.827812\n",
      "Train Epoch: 5 [45000/50000 (90%)]\tLoss: 1795.091250\n",
      "Train Epoch: 5 [46000/50000 (92%)]\tLoss: 1785.417500\n",
      "Train Epoch: 5 [47000/50000 (94%)]\tLoss: 1766.394844\n",
      "Train Epoch: 5 [48000/50000 (96%)]\tLoss: 1812.958281\n",
      "Train Epoch: 5 [49000/50000 (98%)]\tLoss: 1721.745312\n",
      "====> Epoch: 5 Average loss: 1789.3236\n",
      "\n",
      "Test set: Average loss: 1779.2616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "main4(load_old=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 1774.257031\n",
      "Train Epoch: 9 [1000/50000 (2%)]\tLoss: 1848.970156\n",
      "Train Epoch: 9 [2000/50000 (4%)]\tLoss: 1804.980937\n",
      "Train Epoch: 9 [3000/50000 (6%)]\tLoss: 1790.766875\n",
      "Train Epoch: 9 [4000/50000 (8%)]\tLoss: 1768.536250\n",
      "Train Epoch: 9 [5000/50000 (10%)]\tLoss: 1853.142031\n",
      "Train Epoch: 9 [6000/50000 (12%)]\tLoss: 1751.790000\n",
      "Train Epoch: 9 [7000/50000 (14%)]\tLoss: 1767.161250\n",
      "Train Epoch: 9 [8000/50000 (16%)]\tLoss: 1825.889844\n",
      "Train Epoch: 9 [9000/50000 (18%)]\tLoss: 1779.174219\n",
      "Train Epoch: 9 [10000/50000 (20%)]\tLoss: 1807.132031\n",
      "Train Epoch: 9 [11000/50000 (22%)]\tLoss: 1811.748125\n",
      "Train Epoch: 9 [12000/50000 (24%)]\tLoss: 1757.272813\n",
      "Train Epoch: 9 [13000/50000 (26%)]\tLoss: 1773.206250\n",
      "Train Epoch: 9 [14000/50000 (28%)]\tLoss: 1751.124531\n",
      "Train Epoch: 9 [15000/50000 (30%)]\tLoss: 1748.310156\n",
      "Train Epoch: 9 [16000/50000 (32%)]\tLoss: 1757.991563\n",
      "Train Epoch: 9 [17000/50000 (34%)]\tLoss: 1790.067031\n",
      "Train Epoch: 9 [18000/50000 (36%)]\tLoss: 1752.407656\n",
      "Train Epoch: 9 [19000/50000 (38%)]\tLoss: 1805.430938\n",
      "Train Epoch: 9 [20000/50000 (40%)]\tLoss: 1766.216719\n",
      "Train Epoch: 9 [21000/50000 (42%)]\tLoss: 1755.501563\n",
      "Train Epoch: 9 [22000/50000 (44%)]\tLoss: 1768.973125\n",
      "Train Epoch: 9 [23000/50000 (46%)]\tLoss: 1750.120312\n",
      "Train Epoch: 9 [24000/50000 (48%)]\tLoss: 1802.320000\n",
      "Train Epoch: 9 [25000/50000 (50%)]\tLoss: 1769.004531\n",
      "Train Epoch: 9 [26000/50000 (52%)]\tLoss: 1765.986094\n",
      "Train Epoch: 9 [27000/50000 (54%)]\tLoss: 1813.057656\n",
      "Train Epoch: 9 [28000/50000 (56%)]\tLoss: 1746.307656\n",
      "Train Epoch: 9 [29000/50000 (58%)]\tLoss: 1779.041563\n",
      "Train Epoch: 9 [30000/50000 (60%)]\tLoss: 1784.851719\n",
      "Train Epoch: 9 [31000/50000 (62%)]\tLoss: 1767.298906\n",
      "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 1814.852500\n",
      "Train Epoch: 9 [33000/50000 (66%)]\tLoss: 1803.069062\n",
      "Train Epoch: 9 [34000/50000 (68%)]\tLoss: 1749.276875\n",
      "Train Epoch: 9 [35000/50000 (70%)]\tLoss: 1796.128125\n",
      "Train Epoch: 9 [36000/50000 (72%)]\tLoss: 1779.650312\n",
      "Train Epoch: 9 [37000/50000 (74%)]\tLoss: 1755.806562\n",
      "Train Epoch: 9 [38000/50000 (76%)]\tLoss: 1773.898594\n",
      "Train Epoch: 9 [39000/50000 (78%)]\tLoss: 1729.794531\n",
      "Train Epoch: 9 [40000/50000 (80%)]\tLoss: 1805.734375\n",
      "Train Epoch: 9 [41000/50000 (82%)]\tLoss: 1779.013281\n",
      "Train Epoch: 9 [42000/50000 (84%)]\tLoss: 1754.354375\n",
      "Train Epoch: 9 [43000/50000 (86%)]\tLoss: 1754.212812\n",
      "Train Epoch: 9 [44000/50000 (88%)]\tLoss: 1806.619219\n",
      "Train Epoch: 9 [45000/50000 (90%)]\tLoss: 1806.386406\n",
      "Train Epoch: 9 [46000/50000 (92%)]\tLoss: 1793.861875\n",
      "Train Epoch: 9 [47000/50000 (94%)]\tLoss: 1800.718437\n",
      "Train Epoch: 9 [48000/50000 (96%)]\tLoss: 1813.450625\n",
      "Train Epoch: 9 [49000/50000 (98%)]\tLoss: 1786.717344\n",
      "====> Epoch: 9 Average loss: 1786.5796\n",
      "\n",
      "Test set: Average loss: 1774.4809\n",
      "\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1797.279844\n",
      "Train Epoch: 10 [1000/50000 (2%)]\tLoss: 1734.559531\n",
      "Train Epoch: 10 [2000/50000 (4%)]\tLoss: 1793.613750\n",
      "Train Epoch: 10 [3000/50000 (6%)]\tLoss: 1751.587969\n",
      "Train Epoch: 10 [4000/50000 (8%)]\tLoss: 1740.880469\n",
      "Train Epoch: 10 [5000/50000 (10%)]\tLoss: 1778.501719\n",
      "Train Epoch: 10 [6000/50000 (12%)]\tLoss: 1795.547344\n",
      "Train Epoch: 10 [7000/50000 (14%)]\tLoss: 1745.283750\n",
      "Train Epoch: 10 [8000/50000 (16%)]\tLoss: 1772.988906\n",
      "Train Epoch: 10 [9000/50000 (18%)]\tLoss: 1765.568594\n",
      "Train Epoch: 10 [10000/50000 (20%)]\tLoss: 1773.844687\n",
      "Train Epoch: 10 [11000/50000 (22%)]\tLoss: 1828.397813\n",
      "Train Epoch: 10 [12000/50000 (24%)]\tLoss: 1779.674687\n",
      "Train Epoch: 10 [13000/50000 (26%)]\tLoss: 1784.477500\n",
      "Train Epoch: 10 [14000/50000 (28%)]\tLoss: 1731.947500\n",
      "Train Epoch: 10 [15000/50000 (30%)]\tLoss: 1775.764688\n",
      "Train Epoch: 10 [16000/50000 (32%)]\tLoss: 1777.975156\n",
      "Train Epoch: 10 [17000/50000 (34%)]\tLoss: 1774.574219\n",
      "Train Epoch: 10 [18000/50000 (36%)]\tLoss: 1779.315781\n",
      "Train Epoch: 10 [19000/50000 (38%)]\tLoss: 1751.244531\n",
      "Train Epoch: 10 [20000/50000 (40%)]\tLoss: 1781.215469\n",
      "Train Epoch: 10 [21000/50000 (42%)]\tLoss: 1728.525156\n",
      "Train Epoch: 10 [22000/50000 (44%)]\tLoss: 1779.104375\n",
      "Train Epoch: 10 [23000/50000 (46%)]\tLoss: 1807.624063\n",
      "Train Epoch: 10 [24000/50000 (48%)]\tLoss: 1785.616719\n",
      "Train Epoch: 10 [25000/50000 (50%)]\tLoss: 1779.574063\n",
      "Train Epoch: 10 [26000/50000 (52%)]\tLoss: 1796.685000\n",
      "Train Epoch: 10 [27000/50000 (54%)]\tLoss: 1732.702969\n",
      "Train Epoch: 10 [28000/50000 (56%)]\tLoss: 1789.450469\n",
      "Train Epoch: 10 [29000/50000 (58%)]\tLoss: 1784.035156\n",
      "Train Epoch: 10 [30000/50000 (60%)]\tLoss: 1785.730313\n",
      "Train Epoch: 10 [31000/50000 (62%)]\tLoss: 1788.110312\n",
      "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1772.488750\n",
      "Train Epoch: 10 [33000/50000 (66%)]\tLoss: 1715.637344\n",
      "Train Epoch: 10 [34000/50000 (68%)]\tLoss: 1758.728281\n",
      "Train Epoch: 10 [35000/50000 (70%)]\tLoss: 1810.595313\n",
      "Train Epoch: 10 [36000/50000 (72%)]\tLoss: 1749.813437\n",
      "Train Epoch: 10 [37000/50000 (74%)]\tLoss: 1802.616250\n",
      "Train Epoch: 10 [38000/50000 (76%)]\tLoss: 1811.823750\n",
      "Train Epoch: 10 [39000/50000 (78%)]\tLoss: 1724.232812\n",
      "Train Epoch: 10 [40000/50000 (80%)]\tLoss: 1772.899844\n",
      "Train Epoch: 10 [41000/50000 (82%)]\tLoss: 1789.180000\n",
      "Train Epoch: 10 [42000/50000 (84%)]\tLoss: 1753.836719\n",
      "Train Epoch: 10 [43000/50000 (86%)]\tLoss: 1770.789687\n",
      "Train Epoch: 10 [44000/50000 (88%)]\tLoss: 1795.588594\n",
      "Train Epoch: 10 [45000/50000 (90%)]\tLoss: 1767.922031\n",
      "Train Epoch: 10 [46000/50000 (92%)]\tLoss: 1737.301875\n",
      "Train Epoch: 10 [47000/50000 (94%)]\tLoss: 1780.366250\n",
      "Train Epoch: 10 [48000/50000 (96%)]\tLoss: 1749.938594\n",
      "Train Epoch: 10 [49000/50000 (98%)]\tLoss: 1718.201562\n",
      "====> Epoch: 10 Average loss: 1772.5575\n",
      "\n",
      "Test set: Average loss: 1771.5100\n",
      "\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 1760.693125\n",
      "Train Epoch: 11 [1000/50000 (2%)]\tLoss: 1757.941719\n",
      "Train Epoch: 11 [2000/50000 (4%)]\tLoss: 1748.507188\n",
      "Train Epoch: 11 [3000/50000 (6%)]\tLoss: 1760.735938\n",
      "Train Epoch: 11 [4000/50000 (8%)]\tLoss: 1781.919688\n",
      "Train Epoch: 11 [5000/50000 (10%)]\tLoss: 1752.442969\n",
      "Train Epoch: 11 [6000/50000 (12%)]\tLoss: 1800.688125\n",
      "Train Epoch: 11 [7000/50000 (14%)]\tLoss: 1777.920156\n",
      "Train Epoch: 11 [8000/50000 (16%)]\tLoss: 1755.968437\n",
      "Train Epoch: 11 [9000/50000 (18%)]\tLoss: 1787.194062\n",
      "Train Epoch: 11 [10000/50000 (20%)]\tLoss: 1726.235625\n",
      "Train Epoch: 11 [11000/50000 (22%)]\tLoss: 1769.540781\n",
      "Train Epoch: 11 [12000/50000 (24%)]\tLoss: 1791.081406\n",
      "Train Epoch: 11 [13000/50000 (26%)]\tLoss: 1782.110469\n",
      "Train Epoch: 11 [14000/50000 (28%)]\tLoss: 1784.945000\n",
      "Train Epoch: 11 [15000/50000 (30%)]\tLoss: 1763.617500\n",
      "Train Epoch: 11 [16000/50000 (32%)]\tLoss: 1775.552187\n",
      "Train Epoch: 11 [17000/50000 (34%)]\tLoss: 1799.476406\n",
      "Train Epoch: 11 [18000/50000 (36%)]\tLoss: 1799.184375\n",
      "Train Epoch: 11 [19000/50000 (38%)]\tLoss: 1778.027500\n",
      "Train Epoch: 11 [20000/50000 (40%)]\tLoss: 1762.396875\n",
      "Train Epoch: 11 [21000/50000 (42%)]\tLoss: 1752.020469\n",
      "Train Epoch: 11 [22000/50000 (44%)]\tLoss: 1807.610156\n",
      "Train Epoch: 11 [23000/50000 (46%)]\tLoss: 1774.739531\n",
      "Train Epoch: 11 [24000/50000 (48%)]\tLoss: 1728.951250\n",
      "Train Epoch: 11 [25000/50000 (50%)]\tLoss: 1783.171562\n",
      "Train Epoch: 11 [26000/50000 (52%)]\tLoss: 1795.516562\n",
      "Train Epoch: 11 [27000/50000 (54%)]\tLoss: 1773.316719\n",
      "Train Epoch: 11 [28000/50000 (56%)]\tLoss: 1766.734375\n",
      "Train Epoch: 11 [29000/50000 (58%)]\tLoss: 1763.363594\n",
      "Train Epoch: 11 [30000/50000 (60%)]\tLoss: 1781.972344\n",
      "Train Epoch: 11 [31000/50000 (62%)]\tLoss: 1816.083125\n",
      "Train Epoch: 11 [32000/50000 (64%)]\tLoss: 1724.298125\n",
      "Train Epoch: 11 [33000/50000 (66%)]\tLoss: 1744.742344\n",
      "Train Epoch: 11 [34000/50000 (68%)]\tLoss: 1748.415937\n",
      "Train Epoch: 11 [35000/50000 (70%)]\tLoss: 1781.659688\n",
      "Train Epoch: 11 [36000/50000 (72%)]\tLoss: 1757.415000\n",
      "Train Epoch: 11 [37000/50000 (74%)]\tLoss: 1802.041094\n",
      "Train Epoch: 11 [38000/50000 (76%)]\tLoss: 1756.200000\n",
      "Train Epoch: 11 [39000/50000 (78%)]\tLoss: 1779.462500\n",
      "Train Epoch: 11 [40000/50000 (80%)]\tLoss: 1784.413438\n",
      "Train Epoch: 11 [41000/50000 (82%)]\tLoss: 1776.091875\n",
      "Train Epoch: 11 [42000/50000 (84%)]\tLoss: 1715.054375\n",
      "Train Epoch: 11 [43000/50000 (86%)]\tLoss: 1769.699844\n",
      "Train Epoch: 11 [44000/50000 (88%)]\tLoss: 1768.537031\n",
      "Train Epoch: 11 [45000/50000 (90%)]\tLoss: 1773.894063\n",
      "Train Epoch: 11 [46000/50000 (92%)]\tLoss: 1771.278438\n",
      "Train Epoch: 11 [47000/50000 (94%)]\tLoss: 1765.399375\n",
      "Train Epoch: 11 [48000/50000 (96%)]\tLoss: 1748.999375\n",
      "Train Epoch: 11 [49000/50000 (98%)]\tLoss: 1788.584531\n",
      "====> Epoch: 11 Average loss: 1770.3288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1768.2175\n",
      "\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 1771.018125\n",
      "Train Epoch: 12 [1000/50000 (2%)]\tLoss: 1751.786719\n",
      "Train Epoch: 12 [2000/50000 (4%)]\tLoss: 1761.560469\n",
      "Train Epoch: 12 [3000/50000 (6%)]\tLoss: 1768.295000\n",
      "Train Epoch: 12 [4000/50000 (8%)]\tLoss: 1791.302031\n",
      "Train Epoch: 12 [5000/50000 (10%)]\tLoss: 1761.367812\n",
      "Train Epoch: 12 [6000/50000 (12%)]\tLoss: 1748.582344\n",
      "Train Epoch: 12 [7000/50000 (14%)]\tLoss: 1790.784531\n",
      "Train Epoch: 12 [8000/50000 (16%)]\tLoss: 1763.949063\n",
      "Train Epoch: 12 [9000/50000 (18%)]\tLoss: 1767.829531\n",
      "Train Epoch: 12 [10000/50000 (20%)]\tLoss: 1799.593281\n",
      "Train Epoch: 12 [11000/50000 (22%)]\tLoss: 1797.942188\n",
      "Train Epoch: 12 [12000/50000 (24%)]\tLoss: 1794.568438\n",
      "Train Epoch: 12 [13000/50000 (26%)]\tLoss: 1731.824687\n",
      "Train Epoch: 12 [14000/50000 (28%)]\tLoss: 1772.903125\n",
      "Train Epoch: 12 [15000/50000 (30%)]\tLoss: 1761.394844\n",
      "Train Epoch: 12 [16000/50000 (32%)]\tLoss: 1766.226719\n",
      "Train Epoch: 12 [17000/50000 (34%)]\tLoss: 1778.708750\n",
      "Train Epoch: 12 [18000/50000 (36%)]\tLoss: 1749.186563\n",
      "Train Epoch: 12 [19000/50000 (38%)]\tLoss: 1764.955469\n",
      "Train Epoch: 12 [20000/50000 (40%)]\tLoss: 1744.345469\n",
      "Train Epoch: 12 [21000/50000 (42%)]\tLoss: 1723.046875\n",
      "Train Epoch: 12 [22000/50000 (44%)]\tLoss: 1753.489375\n",
      "Train Epoch: 12 [23000/50000 (46%)]\tLoss: 1755.624063\n",
      "Train Epoch: 12 [24000/50000 (48%)]\tLoss: 1775.887187\n",
      "Train Epoch: 12 [25000/50000 (50%)]\tLoss: 1747.893906\n",
      "Train Epoch: 12 [26000/50000 (52%)]\tLoss: 1713.162656\n",
      "Train Epoch: 12 [27000/50000 (54%)]\tLoss: 1763.016406\n",
      "Train Epoch: 12 [28000/50000 (56%)]\tLoss: 1776.199375\n",
      "Train Epoch: 12 [29000/50000 (58%)]\tLoss: 1792.295156\n",
      "Train Epoch: 12 [30000/50000 (60%)]\tLoss: 1789.648438\n",
      "Train Epoch: 12 [31000/50000 (62%)]\tLoss: 1788.908906\n",
      "Train Epoch: 12 [32000/50000 (64%)]\tLoss: 1837.215781\n",
      "Train Epoch: 12 [33000/50000 (66%)]\tLoss: 1756.699531\n",
      "Train Epoch: 12 [34000/50000 (68%)]\tLoss: 1749.566875\n",
      "Train Epoch: 12 [35000/50000 (70%)]\tLoss: 1786.285938\n",
      "Train Epoch: 12 [36000/50000 (72%)]\tLoss: 1769.354375\n",
      "Train Epoch: 12 [37000/50000 (74%)]\tLoss: 1758.288438\n",
      "Train Epoch: 12 [38000/50000 (76%)]\tLoss: 1805.772813\n",
      "Train Epoch: 12 [39000/50000 (78%)]\tLoss: 1801.703125\n",
      "Train Epoch: 12 [40000/50000 (80%)]\tLoss: 1738.633750\n",
      "Train Epoch: 12 [41000/50000 (82%)]\tLoss: 1748.199687\n",
      "Train Epoch: 12 [42000/50000 (84%)]\tLoss: 1771.166406\n",
      "Train Epoch: 12 [43000/50000 (86%)]\tLoss: 1750.235781\n",
      "Train Epoch: 12 [44000/50000 (88%)]\tLoss: 1791.201406\n",
      "Train Epoch: 12 [45000/50000 (90%)]\tLoss: 1747.297500\n",
      "Train Epoch: 12 [46000/50000 (92%)]\tLoss: 1748.987656\n",
      "Train Epoch: 12 [47000/50000 (94%)]\tLoss: 1768.068594\n",
      "Train Epoch: 12 [48000/50000 (96%)]\tLoss: 1704.621719\n",
      "Train Epoch: 12 [49000/50000 (98%)]\tLoss: 1756.487344\n",
      "====> Epoch: 12 Average loss: 1768.4713\n",
      "\n",
      "Test set: Average loss: 1771.3559\n",
      "\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 1716.238437\n",
      "Train Epoch: 13 [1000/50000 (2%)]\tLoss: 1739.282031\n",
      "Train Epoch: 13 [2000/50000 (4%)]\tLoss: 1763.970000\n",
      "Train Epoch: 13 [3000/50000 (6%)]\tLoss: 1718.894375\n",
      "Train Epoch: 13 [4000/50000 (8%)]\tLoss: 1736.749219\n",
      "Train Epoch: 13 [5000/50000 (10%)]\tLoss: 1767.075625\n",
      "Train Epoch: 13 [6000/50000 (12%)]\tLoss: 1753.556094\n",
      "Train Epoch: 13 [7000/50000 (14%)]\tLoss: 1820.210781\n",
      "Train Epoch: 13 [8000/50000 (16%)]\tLoss: 1754.282969\n",
      "Train Epoch: 13 [9000/50000 (18%)]\tLoss: 1763.786406\n",
      "Train Epoch: 13 [10000/50000 (20%)]\tLoss: 1772.356875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-cb6a50a0d902>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmain4\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_old\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-207c0ca9636e>\u001b[0m in \u001b[0;36mmain4\u001b[1;34m(load_old)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-207c0ca9636e>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args, model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\thisenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-207c0ca9636e>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m             \u001b[1;31m#return F.log_softmax(x, dim=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mmu_0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-207c0ca9636e>\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;31m#x = F.max_pool2d(x, 2, 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[1;31m#x = F.max_pool2d(x, 2, 2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\thisenv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m    860\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "main4(load_old=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
